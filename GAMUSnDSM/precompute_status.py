#!/usr/bin/env python3
"""
é¢„è®¡ç®—GAMUSæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯è„šæœ¬
è®¡ç®—å…¨é‡æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜ï¼Œç”¨äºåç»­è®­ç»ƒæ—¶å¿«é€ŸåŠ è½½
"""

import os
import json
import numpy as np
import cv2
import logging
import argparse
import time
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

def setup_logger(log_path: Optional[str] = None, log_level: str = 'INFO'):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logger = logging.getLogger('precompute_stats')
    logger.setLevel(getattr(logging, log_level))
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
    if log_path:
        os.makedirs(os.path.dirname(log_path), exist_ok=True)
        file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

def validate_directories(data_dir: str, logger: logging.Logger) -> Dict[str, Dict]:
    """éªŒè¯æ•°æ®ç›®å½•ç»“æ„"""
    logger.info("ğŸ” éªŒè¯æ•°æ®ç›®å½•ç»“æ„...")
    
    subdirs = [
        ('train/images', 'training images'),
        ('train/depths', 'training depths'),
        ('val/images', 'validation images'),
        ('val/depths', 'validation depths')
    ]
    
    results = {}
    
    for subdir, desc in subdirs:
        full_path = os.path.join(data_dir, subdir)
        
        if os.path.exists(full_path):
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶
            files = [f for f in os.listdir(full_path) 
                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))]
            
            results[subdir] = {
                'exists': True,
                'path': full_path,
                'files': files,
                'count': len(files)
            }
            
            logger.info(f"  âœ“ {desc}: {len(files)} ä¸ªæ–‡ä»¶")
            if len(files) > 0:
                logger.info(f"    ç¤ºä¾‹: {files[0]}")
                
        else:
            results[subdir] = {
                'exists': False,
                'path': full_path,
                'files': [],
                'count': 0
            }
            logger.warning(f"  âš ï¸  {desc}: ç›®å½•ä¸å­˜åœ¨")
    
    return results

def extract_base_name(filename: str, is_image: bool = True) -> Optional[str]:
    """æå–æ–‡ä»¶çš„åŸºç¡€åç§°ç”¨äºåŒ¹é…"""
    try:
        # å¤„ç†ä¸åŒçš„å‘½åæ¨¡å¼
        if '_RGB_' in filename and is_image:
            base_name = filename.replace('_RGB_', '_').rsplit('.', 1)[0]
        elif '_AGL_' in filename and not is_image:
            base_name = filename.replace('_AGL_', '_').rsplit('.', 1)[0]
        elif '_DSM_' in filename and not is_image:
            base_name = filename.replace('_DSM_', '_').rsplit('.', 1)[0]
        elif '_depth_' in filename and not is_image:
            base_name = filename.replace('_depth_', '_').rsplit('.', 1)[0]
        else:
            # é€šç”¨å¤„ç†ï¼šå»æ‰æ‰©å±•å
            base_name = Path(filename).stem
        
        return base_name
    except Exception:
        return None

def match_file_pairs(image_files: List[str], depth_files: List[str], logger: logging.Logger) -> List[Tuple[str, str]]:
    """åŒ¹é…å›¾åƒå’Œæ·±åº¦æ–‡ä»¶å¯¹"""
    logger.info("ğŸ”— åŒ¹é…å›¾åƒå’Œæ·±åº¦æ–‡ä»¶...")
    
    # åˆ›å»ºæ˜ å°„å­—å…¸
    image_dict = {}
    depth_dict = {}
    
    # å¤„ç†å›¾åƒæ–‡ä»¶
    for img_file in image_files:
        base_name = extract_base_name(img_file, is_image=True)
        if base_name:
            image_dict[base_name] = img_file
    
    # å¤„ç†æ·±åº¦æ–‡ä»¶
    for depth_file in depth_files:
        base_name = extract_base_name(depth_file, is_image=False)
        if base_name:
            depth_dict[base_name] = depth_file
    
    # åŒ¹é…æ–‡ä»¶å¯¹
    matched_pairs = []
    for base_name in image_dict:
        if base_name in depth_dict:
            matched_pairs.append((image_dict[base_name], depth_dict[base_name]))
    
    logger.info(f"  âœ“ æˆåŠŸåŒ¹é…: {len(matched_pairs)} å¯¹æ–‡ä»¶")
    logger.info(f"  âš ï¸  æœªåŒ¹é…å›¾åƒ: {len(image_files) - len(matched_pairs)} ä¸ª")
    logger.info(f"  âš ï¸  æœªåŒ¹é…æ·±åº¦: {len(depth_files) - len(matched_pairs)} ä¸ª")
    
    return matched_pairs

def get_valid_mask(data: np.ndarray) -> np.ndarray:
    """è·å–æœ‰æ•ˆæ•°æ®æ©ç """
    invalid_values = [-9999, -32768, 9999, 32767]
    valid_mask = ~(np.isnan(data) | np.isinf(data))
    
    for invalid_val in invalid_values:
        valid_mask = valid_mask & (data != invalid_val)
    
    return valid_mask

def compute_full_statistics(depth_dir: str, 
                          file_pairs: List[Tuple[str, str]], 
                          height_filter: Dict[str, float],
                          logger: logging.Logger) -> Dict:
    """è®¡ç®—å…¨é‡æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    logger.info("ğŸ“Š å¼€å§‹è®¡ç®—å…¨é‡æ•°æ®ç»Ÿè®¡ä¿¡æ¯...")
    logger.info(f"  æ€»æ–‡ä»¶æ•°: {len(file_pairs)}")
    logger.info(f"  é«˜åº¦è¿‡æ»¤èŒƒå›´: [{height_filter['min_height']:.1f}, {height_filter['max_height']:.1f}] ç±³")
    
    all_heights = []
    processed_count = 0
    error_count = 0
    total_pixels = 0
    valid_pixels = 0
    
    start_time = time.time()
    
    # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ‰€æœ‰æ–‡ä»¶
    with tqdm(total=len(file_pairs), desc="å¤„ç†æ·±åº¦æ–‡ä»¶", unit="æ–‡ä»¶") as pbar:
        for idx, (image_file, depth_file) in enumerate(file_pairs):
            try:
                depth_path = os.path.join(depth_dir, depth_file)
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.set_postfix({'å½“å‰': depth_file[:30] + '...' if len(depth_file) > 30 else depth_file})
                
                # åŠ è½½æ·±åº¦æ•°æ®
                depth_data = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)
                
                if depth_data is None:
                    error_count += 1
                    logger.debug(f"æ— æ³•åŠ è½½: {depth_file}")
                    pbar.update(1)
                    continue
                
                # è½¬æ¢æ•°æ®ç±»å‹
                depth_data = depth_data.astype(np.float32)
                total_pixels += depth_data.size
                
                # è¿‡æ»¤æ— æ•ˆå€¼
                valid_mask = get_valid_mask(depth_data)
                valid_heights = depth_data[valid_mask]
                valid_pixels += len(valid_heights)
                
                if len(valid_heights) > 0:
                    # å•ä½è½¬æ¢ï¼šå˜ç±³è½¬ç±³ï¼ˆå‡è®¾åŸå§‹æ•°æ®æ˜¯å˜ç±³ï¼‰
                    valid_heights = valid_heights / 100.0
                    
                    # åº”ç”¨é«˜åº¦è¿‡æ»¤
                    filtered_heights = valid_heights[
                        (valid_heights >= height_filter['min_height']) & 
                        (valid_heights <= height_filter['max_height'])
                    ]
                    
                    if len(filtered_heights) > 0:
                        all_heights.append(filtered_heights)
                        processed_count += 1
                
                pbar.update(1)
                
                # æ¯å¤„ç†1000ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡ä¸­é—´çŠ¶æ€
                if (idx + 1) % 1000 == 0:
                    current_samples = sum(len(h) for h in all_heights)
                    elapsed = time.time() - start_time
                    speed = (idx + 1) / elapsed
                    pbar.set_description(f"å·²é‡‡é›† {current_samples:,} æ•°æ®ç‚¹ ({speed:.1f} æ–‡ä»¶/ç§’)")
                
            except Exception as e:
                error_count += 1
                logger.debug(f"å¤„ç†æ–‡ä»¶ {depth_file} å¤±è´¥: {e}")
                pbar.update(1)
                continue
    
    processing_time = time.time() - start_time
    
    # å¤„ç†ç»“æœ
    logger.info(f"ğŸ“ˆ å¤„ç†å®Œæˆ (è€—æ—¶ {processing_time:.1f}s):")
    logger.info(f"  æˆåŠŸå¤„ç†: {processed_count}/{len(file_pairs)} ä¸ªæ–‡ä»¶ ({processed_count/len(file_pairs)*100:.1f}%)")
    logger.info(f"  é”™è¯¯æ–‡ä»¶: {error_count} ä¸ª")
    logger.info(f"  æ€»åƒç´ æ•°: {total_pixels:,}")
    logger.info(f"  æœ‰æ•ˆåƒç´ : {valid_pixels:,} ({valid_pixels/total_pixels*100:.1f}%)")
    
    if not all_heights:
        raise ValueError("âŒ æ— æ³•æå–æœ‰æ•ˆçš„é«˜åº¦æ•°æ®")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    logger.info("ğŸ”„ åˆå¹¶å¹¶è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯...")
    combined_heights = np.concatenate(all_heights)
    total_samples = len(combined_heights)
    
    logger.info(f"  åˆå¹¶æ•°æ®ç‚¹: {total_samples:,} ä¸ª")
    
    # è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    global_min = float(np.min(combined_heights))
    global_max = float(np.max(combined_heights))
    global_mean = float(np.mean(combined_heights))
    global_std = float(np.std(combined_heights))
    global_median = float(np.median(combined_heights))
    
    # è®¡ç®—åˆ†ä½æ•°
    percentiles = np.percentile(combined_heights, [1, 5, 25, 50, 75, 95, 99])
    percentile_dict = {
        'p1': float(percentiles[0]),
        'p5': float(percentiles[1]),
        'p25': float(percentiles[2]),
        'p50': float(percentiles[3]),
        'p75': float(percentiles[4]),
        'p95': float(percentiles[5]),
        'p99': float(percentiles[6])
    }
    
    # è®¡ç®—ç›´æ–¹å›¾
    hist, bin_edges = np.histogram(combined_heights, bins=100)
    hist_dict = {
        'counts': hist.tolist(),
        'bin_edges': bin_edges.tolist()
    }
    
    # ç»Ÿè®¡ç»“æœ
    stats = {
        'processing_info': {
            'total_files': len(file_pairs),
            'processed_files': processed_count,
            'error_files': error_count,
            'processing_time': processing_time,
            'total_pixels': total_pixels,
            'valid_pixels': valid_pixels,
            'height_filter': height_filter,
            'timestamp': datetime.now().isoformat()
        },
        'global_statistics': {
            'min': global_min,
            'max': global_max,
            'mean': global_mean,
            'std': global_std,
            'median': global_median,
            'range': global_max - global_min,
            'total_samples': total_samples
        },
        'percentiles': percentile_dict,
        'histogram': hist_dict
    }
    
    # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    logger.info("âœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ:")
    logger.info(f"  ğŸ“Š é«˜åº¦èŒƒå›´: [{global_min:.2f}, {global_max:.2f}] ç±³")
    logger.info(f"  ğŸ“Š å‡å€¼Â±æ ‡å‡†å·®: {global_mean:.2f} Â± {global_std:.2f} ç±³")
    logger.info(f"  ğŸ“Š ä¸­ä½æ•°: {global_median:.2f} ç±³")
    logger.info(f"  ğŸ“Š åˆ†ä½æ•° [5%, 25%, 75%, 95%]: {[percentile_dict['p5'], percentile_dict['p25'], percentile_dict['p75'], percentile_dict['p95']]}")
    logger.info(f"  ğŸ“Š æœ‰æ•ˆæ•°æ®ç‚¹: {total_samples:,} ä¸ª")
    
    return stats

def save_statistics(stats: Dict, output_path: str, logger: logging.Logger):
    """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {output_path}")
        
        # è¾“å‡ºæ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_path)
        logger.info(f"  æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description='é¢„è®¡ç®—GAMUSæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯')
    
    parser.add_argument('--data_dir', type=str, help='æ•°æ®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--output', type=str, default='./gamus_full_stats.json',
                       help='è¾“å‡ºç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log_file', type=str, default='./precompute_stats.log',
                       help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--min_height', type=float, default=-5.0,
                       help='æœ€å°é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--max_height', type=float, default=200.0,
                       help='æœ€å¤§é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args.log_file, args.log_level)
    
    logger.info("ğŸš€ GAMUSæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯é¢„è®¡ç®—")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    logger.info(f"é«˜åº¦èŒƒå›´: [{args.min_height}, {args.max_height}] ç±³")
    
    try:
        # 1. éªŒè¯æ•°æ®ç›®å½•
        if not os.path.exists(args.data_dir):
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        
        dir_results = validate_directories(args.data_dir, logger)
        
        # 2. æ£€æŸ¥è®­ç»ƒæ•°æ®
        train_images = dir_results.get('train/images', {})
        train_depths = dir_results.get('train/depths', {})
        
        if not train_images['exists'] or not train_depths['exists']:
            raise ValueError("ç¼ºå°‘è®­ç»ƒæ•°æ®ç›®å½•")
        
        if train_images['count'] == 0 or train_depths['count'] == 0:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©º")
        
        # 3. åŒ¹é…æ–‡ä»¶å¯¹
        file_pairs = match_file_pairs(
            train_images['files'], 
            train_depths['files'], 
            logger
        )
        
        if len(file_pairs) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å›¾åƒ-æ·±åº¦æ–‡ä»¶å¯¹")
        
        # 4. è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨
        height_filter = {
            'min_height': args.min_height,
            'max_height': args.max_height
        }
        
        # 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = compute_full_statistics(
            train_depths['path'],
            file_pairs,
            height_filter,
            logger
        )
        
        # 6. ä¿å­˜ç»“æœ
        save_statistics(stats, args.output, logger)
        
        # 7. æ€»ç»“
        logger.info("=" * 60)
        logger.info("ğŸ‰ ç»Ÿè®¡ä¿¡æ¯é¢„è®¡ç®—å®Œæˆ!")
        logger.info(f"  å¤„ç†æ–‡ä»¶: {len(file_pairs)} å¯¹")
        logger.info(f"  æ•°æ®ç‚¹æ•°: {stats['global_statistics']['total_samples']:,}")
        logger.info(f"  è¾“å‡ºæ–‡ä»¶: {args.output}")
        logger.info("")
        logger.info("ğŸ’¡ ç°åœ¨ä½ å¯ä»¥åœ¨è®­ç»ƒæ—¶ä½¿ç”¨è¿™ä¸ªç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶:")
        logger.info(f"   python train.py --stats_json_path {args.output} ...")
        
    except Exception as e:
        logger.error(f"âŒ é¢„è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())