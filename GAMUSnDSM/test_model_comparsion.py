#!/usr/bin/env python3
"""
å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬ï¼ˆå¤§æ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
æ”¯æŒGAMUSå’ŒDepth2Elevationæ¨¡å‹ï¼Œæ”¯æŒmaskåŠŸèƒ½
ç”¨äºè¯„ä¼°å·²è®­ç»ƒæ¨¡å‹çš„ç²¾åº¦ï¼Œé’ˆå¯¹12000+æ ·æœ¬è¿›è¡Œå†…å­˜å’Œé€Ÿåº¦ä¼˜åŒ–
"""

import os
import torch
import torch.nn.functional as F
import numpy as np
import cv2
import logging
import argparse
import time
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
import gc

# å¯¼å…¥æ›´æ–°åçš„æ¨¡å—
from improved_dataset_with_mask import create_gamus_dataloader
from improved_normalization_loss import create_height_loss
from model_with_comparison import create_gamus_model

def setup_logger(log_path):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    console_handler = logging.StreamHandler()
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger

def load_trained_model(checkpoint_path, device, logger):
    """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
    logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # æå–æ¨¡å‹å‚æ•°å’Œç±»å‹ä¿¡æ¯
        if 'model_state_dict' in checkpoint:
            model_state_dict = checkpoint['model_state_dict']
            model_type = checkpoint.get('model_type', 'gamus')  # è·å–æ¨¡å‹ç±»å‹
            logger.info(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
            logger.info(f"  æ¨¡å‹ç±»å‹: {model_type}")
            logger.info(f"  è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'N/A')}")
            if isinstance(checkpoint.get('loss'), (int, float)):
                logger.info(f"  éªŒè¯æŸå¤±: {checkpoint.get('loss'):.6f}")
        else:
            model_state_dict = checkpoint
            model_type = 'gamus'  # é»˜è®¤ä¸ºgamus
            logger.info("æ£€æŸ¥ç‚¹ä¸ºç›´æ¥çš„æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œå‡è®¾ä¸ºGAMUSæ¨¡å‹")
        
        return model_state_dict, model_type
        
    except Exception as e:
        logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        raise

def create_test_dataset(data_dir, args, logger):
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆæ”¯æŒmaskï¼‰"""
    logger.info("åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    
    # æµ‹è¯•é›†è·¯å¾„
    test_image_dir = os.path.join(data_dir, 'test','images')
    test_label_dir = os.path.join(data_dir, 'test','depths')
    test_mask_dir = None
    
    if args.mask_dir:
        test_mask_dir = os.path.join(args.mask_dir, 'test', 'classes')
    
    # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„æµ‹è¯•é›†ï¼Œä½¿ç”¨éªŒè¯é›†
    if not os.path.exists(test_image_dir):
        test_image_dir = os.path.join(data_dir, 'val','images')
        test_label_dir = os.path.join(data_dir, 'val','depths')
        if args.mask_dir:
            test_mask_dir = os.path.join(args.mask_dir, 'val', 'classes')
        logger.info("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œä½¿ç”¨éªŒè¯é›†è¿›è¡Œæµ‹è¯•")
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†
    if not os.path.exists(test_image_dir):
        test_image_dir = os.path.join(data_dir, 'train','images')
        test_label_dir = os.path.join(data_dir, 'train','depths')
        if args.mask_dir:
            test_mask_dir = os.path.join(args.mask_dir, 'train', 'classes')
        logger.info("æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œæµ‹è¯•ï¼ˆæ³¨æ„ï¼šè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆçš„ç»“æœï¼‰")
    
    if not os.path.exists(test_image_dir):
        raise FileNotFoundError(f"æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {test_image_dir}")
    if not os.path.exists(test_label_dir):
        raise FileNotFoundError(f"æµ‹è¯•æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {test_label_dir}")
    
    # æ£€æŸ¥maskç›®å½•
    if args.mask_dir and test_mask_dir and not os.path.exists(test_mask_dir):
        logger.warning(f"Maskç›®å½•ä¸å­˜åœ¨: {test_mask_dir}ï¼Œå°†ä¸ä½¿ç”¨mask")
        test_mask_dir = None
    
    logger.info(f"æµ‹è¯•å›¾åƒç›®å½•: {test_image_dir}")
    logger.info(f"æµ‹è¯•æ ‡ç­¾ç›®å½•: {test_label_dir}")
    if test_mask_dir:
        logger.info(f"æµ‹è¯•maskç›®å½•: {test_mask_dir}")
    
    # è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨
    height_filter = {
        'min_height': args.min_height,
        'max_height': args.max_height
    }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒmaskï¼‰
    try:
        test_loader, test_dataset = create_gamus_dataloader(
            image_dir=test_image_dir,
            label_dir=test_label_dir,
            mask_dir=test_mask_dir,
            building_class_id=args.building_class_id,
            tree_class_id=args.tree_class_id,
            # use_all_classes=args.use_all_classes,
            batch_size=args.batch_size,
            shuffle=False,
            normalization_method=args.normalization_method,
            enable_augmentation=False,
            stats_json_path=args.stats_json_path,
            height_filter=height_filter,
            force_recompute=False,
            num_workers=args.num_workers
        )
        
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        # å¦‚æœæµ‹è¯•é›†å¤ªå¤§ï¼Œå¯ä»¥é€‰æ‹©é‡‡æ ·
        if args.max_test_samples > 0 and len(test_dataset) > args.max_test_samples:
            logger.info(f"æµ‹è¯•é›†æ ·æœ¬æ•°({len(test_dataset)})è¶…è¿‡é™åˆ¶({args.max_test_samples})ï¼Œå°†è¿›è¡Œéšæœºé‡‡æ ·")
            # åˆ›å»ºå­é›†
            indices = np.random.choice(len(test_dataset), args.max_test_samples, replace=False)
            test_dataset = torch.utils.data.Subset(test_dataset, indices)
            
            # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
            test_loader = torch.utils.data.DataLoader(
                test_dataset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=torch.cuda.is_available() and not args.disable_pin_memory,
                drop_last=False,
                prefetch_factor=2 if args.num_workers > 0 else 2
            )
            logger.info(f"é‡‡æ ·åæµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    
    except Exception as e:
        logger.error(f"åˆ›å»ºæµ‹è¯•æ•°æ®é›†å¤±è´¥: {e}")
        raise
    
    return test_loader, test_dataset

class OnlineMetricsCalculator:
    """åœ¨çº¿æŒ‡æ ‡è®¡ç®—å™¨ï¼Œé¿å…å­˜å‚¨æ‰€æœ‰æ•°æ®"""
    
    def __init__(self, height_normalizer, sample_for_correlation=5000):
        self.height_normalizer = height_normalizer
        self.sample_for_correlation = sample_for_correlation
        self.reset()
        
    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.count = 0
        self.sum_se = 0.0  # å¹³æ–¹è¯¯å·®å’Œ
        self.sum_ae = 0.0  # ç»å¯¹è¯¯å·®å’Œ
        self.sum_targets = 0.0
        self.sum_targets_sq = 0.0
        self.sum_preds = 0.0
        self.sum_cross = 0.0  # äº¤å‰é¡¹ç”¨äºç›¸å…³æ€§è®¡ç®—
        
        # ç²¾åº¦è®¡æ•°å™¨
        self.accuracy_1m = 0
        self.accuracy_2m = 0
        self.accuracy_5m = 0
        self.accuracy_10m = 0
        
        # åˆ†å±‚è¯¯å·®
        self.ground_errors = []
        self.low_errors = []
        self.mid_errors = []
        self.high_errors = []
        
        # ç›¸å…³æ€§è®¡ç®—çš„é‡‡æ ·æ•°æ®
        self.sampled_preds = []
        self.sampled_targets = []
        
        # æ•°æ®èŒƒå›´
        self.min_target = float('inf')
        self.max_target = float('-inf')
        self.min_pred = float('inf')
        self.max_pred = float('-inf')
    
    def update(self, predictions, targets, masks=None):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¯æŒmaskï¼‰"""
        # åº”ç”¨maskè¿‡æ»¤
        if masks is not None:
            # åªå¤„ç†mask=1ä¸”targetsæœ‰æ•ˆçš„åƒç´ 
            valid_mask = (masks > 0.5) & (targets >= 0)
            if valid_mask.sum() == 0:
                return
            predictions = predictions[valid_mask]
            targets = targets[valid_mask]
        
        # åå½’ä¸€åŒ–åˆ°çœŸå®é«˜åº¦å€¼
        pred_heights = self.height_normalizer.denormalize(predictions.flatten())
        target_heights = self.height_normalizer.denormalize(targets.flatten())
        
        # ç§»é™¤æ— æ•ˆå€¼
        valid_mask = (~np.isnan(pred_heights) & ~np.isnan(target_heights) & 
                     ~np.isinf(pred_heights) & ~np.isinf(target_heights))
        
        if np.sum(valid_mask) == 0:
            return
        
        valid_preds = pred_heights[valid_mask]
        valid_targets = target_heights[valid_mask]
        
        n = len(valid_preds)
        self.count += n
        
        # åŸºç¡€ç»Ÿè®¡
        errors = valid_preds - valid_targets
        abs_errors = np.abs(errors)
        
        self.sum_se += np.sum(errors ** 2)
        self.sum_ae += np.sum(abs_errors)
        self.sum_targets += np.sum(valid_targets)
        self.sum_targets_sq += np.sum(valid_targets ** 2)
        self.sum_preds += np.sum(valid_preds)
        self.sum_cross += np.sum(valid_preds * valid_targets)
        
        # ç²¾åº¦ç»Ÿè®¡
        self.accuracy_1m += np.sum(abs_errors <= 1.0)
        self.accuracy_2m += np.sum(abs_errors <= 2.0)
        self.accuracy_5m += np.sum(abs_errors <= 5.0)
        self.accuracy_10m += np.sum(abs_errors <= 10.0)
        
        # åˆ†å±‚è¯¯å·®ï¼ˆåªå­˜å‚¨å°æ ·æœ¬ï¼‰
        ground_mask = (valid_targets >= -5) & (valid_targets <= 5)
        low_mask = (valid_targets > 5) & (valid_targets <= 20)
        mid_mask = (valid_targets > 20) & (valid_targets <= 50)
        high_mask = valid_targets > 50
        
        if np.sum(ground_mask) > 0:
            self.ground_errors.extend(abs_errors[ground_mask].tolist())
        if np.sum(low_mask) > 0:
            self.low_errors.extend(abs_errors[low_mask].tolist())
        if np.sum(mid_mask) > 0:
            self.mid_errors.extend(abs_errors[mid_mask].tolist())
        if np.sum(high_mask) > 0:
            self.high_errors.extend(abs_errors[high_mask].tolist())
        
        # é™åˆ¶åˆ†å±‚è¯¯å·®æ•°ç»„å¤§å°
        max_layer_samples = 2000
        if len(self.ground_errors) > max_layer_samples:
            self.ground_errors = self.ground_errors[-max_layer_samples:]
        if len(self.low_errors) > max_layer_samples:
            self.low_errors = self.low_errors[-max_layer_samples:]
        if len(self.mid_errors) > max_layer_samples:
            self.mid_errors = self.mid_errors[-max_layer_samples:]
        if len(self.high_errors) > max_layer_samples:
            self.high_errors = self.high_errors[-max_layer_samples:]
        
        # æ•°æ®èŒƒå›´
        self.min_target = min(self.min_target, valid_targets.min())
        self.max_target = max(self.max_target, valid_targets.max())
        self.min_pred = min(self.min_pred, valid_preds.min())
        self.max_pred = max(self.max_pred, valid_preds.max())
        
        # ç›¸å…³æ€§è®¡ç®—çš„é‡‡æ ·
        if len(self.sampled_preds) < self.sample_for_correlation:
            sample_size = min(len(valid_preds), self.sample_for_correlation - len(self.sampled_preds))
            indices = np.random.choice(len(valid_preds), sample_size, replace=False)
            self.sampled_preds.extend(valid_preds[indices].tolist())
            self.sampled_targets.extend(valid_targets[indices].tolist())
    
    def compute_metrics(self):
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        if self.count == 0:
            return {}
        
        # åŸºç¡€æŒ‡æ ‡
        mse = self.sum_se / self.count
        mae = self.sum_ae / self.count
        rmse = np.sqrt(mse)
        
        # RÂ²è®¡ç®—
        mean_target = self.sum_targets / self.count
        ss_tot = self.sum_targets_sq - 2 * mean_target * self.sum_targets + self.count * (mean_target ** 2)
        ss_res = self.sum_se
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-10 else 0.0
        r2 = max(-10, min(1, r2))
        
        # ç›¸å…³æ€§æŒ‡æ ‡
        pearson_r = pearson_p = spearman_r = spearman_p = 0.0
        if len(self.sampled_preds) > 10:
            try:
                from scipy.stats import pearsonr, spearmanr
                if len(np.unique(self.sampled_targets)) > 1 and len(np.unique(self.sampled_preds)) > 1:
                    pearson_r, pearson_p = pearsonr(self.sampled_targets, self.sampled_preds)
                    spearman_r, spearman_p = spearmanr(self.sampled_targets, self.sampled_preds)
            except:
                pass
        
        # ç²¾åº¦æŒ‡æ ‡
        accuracy_1m = self.accuracy_1m / self.count
        accuracy_2m = self.accuracy_2m / self.count
        accuracy_5m = self.accuracy_5m / self.count
        accuracy_10m = self.accuracy_10m / self.count
        
        # ç›¸å¯¹è¯¯å·®ï¼ˆä½¿ç”¨åœ¨çº¿è®¡ç®—çš„å‡å€¼ï¼‰
        mean_target_abs = abs(mean_target) if abs(mean_target) > 0.1 else 0.1
        relative_error = (mae / mean_target_abs) * 100
        
        # åˆ†å±‚è¯¯å·®
        mae_ground = np.mean(self.ground_errors) if self.ground_errors else 0.0
        mae_low = np.mean(self.low_errors) if self.low_errors else 0.0
        mae_mid = np.mean(self.mid_errors) if self.mid_errors else 0.0
        mae_high = np.mean(self.high_errors) if self.high_errors else 0.0
        
        metrics = {
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2),
            'pearson_r': float(pearson_r),
            'pearson_p': float(pearson_p),
            'spearman_r': float(spearman_r),
            'spearman_p': float(spearman_p),
            'height_accuracy_1m': float(accuracy_1m),
            'height_accuracy_2m': float(accuracy_2m),
            'height_accuracy_5m': float(accuracy_5m),
            'height_accuracy_10m': float(accuracy_10m),
            'relative_error': float(relative_error),
            'mae_ground_level': float(mae_ground),
            'mae_low_buildings': float(mae_low),
            'mae_mid_buildings': float(mae_mid),
            'mae_high_buildings': float(mae_high),
            'data_range_min': float(self.min_target),
            'data_range_max': float(self.max_target),
            'prediction_range_min': float(self.min_pred),
            'prediction_range_max': float(self.max_pred),
            'total_samples': self.count
        }
        
        return metrics

def test_model_optimized(model, test_loader, device, logger, criterion=None, memory_cleanup_interval=50):
    """ä¼˜åŒ–çš„æ¨¡å‹æµ‹è¯•å‡½æ•°ï¼ˆæ”¯æŒmaskï¼‰"""
    model.eval()
    logger.info("å¼€å§‹æ¨¡å‹æµ‹è¯•ï¼ˆå¤§æ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒmaskï¼‰...")
    
    # è·å–å½’ä¸€åŒ–å™¨
    if hasattr(test_loader.dataset, 'dataset'):
        # Subsetæƒ…å†µ
        if hasattr(test_loader.dataset.dataset, 'get_normalizer'):
            height_normalizer = test_loader.dataset.dataset.get_normalizer()
        else:
            # ä»åŸå§‹æ•°æ®é›†è·å–
            original_dataset = test_loader.dataset.dataset
            height_normalizer = original_dataset.get_normalizer()
    else:
        height_normalizer = test_loader.dataset.get_normalizer()
    
    metrics_calculator = OnlineMetricsCalculator(height_normalizer)
    
    batch_losses = []
    inference_times = []
    failed_batches = 0
    
    start_time = time.time()
    
    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc='Testing Model')
        
        for batch_idx, batch_data in enumerate(test_pbar):
            try:
                # å¤„ç†å¯èƒ½åŒ…å«maskçš„batchæ•°æ®
                if len(batch_data) == 3:
                    images, targets, masks = batch_data
                    images = images.to(device, non_blocking=True)
                    targets = targets.to(device, non_blocking=True)
                    masks = masks.to(device, non_blocking=True)
                else:
                    images, targets = batch_data
                    images = images.to(device, non_blocking=True)
                    targets = targets.to(device, non_blocking=True)
                    masks = torch.ones_like(targets).to(device)  # å…¨1mask
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                if torch.isnan(images).any() or torch.isinf(images).any():
                    logger.warning(f"æµ‹è¯•æ‰¹æ¬¡{batch_idx}: è¾“å…¥å›¾åƒåŒ…å«NaNæˆ–Inf")
                    failed_batches += 1
                    continue
                    
                if torch.isnan(targets).any() or torch.isinf(targets).any():
                    logger.warning(f"æµ‹è¯•æ‰¹æ¬¡{batch_idx}: nDSMç›®æ ‡åŒ…å«NaNæˆ–Inf")
                    failed_batches += 1
                    continue
                
                # æµ‹é‡æ¨ç†æ—¶é—´
                inference_start = time.time()
                predictions = model(images)
                inference_time = time.time() - inference_start
                inference_times.append(inference_time)
                
                # é¢„æµ‹ç»“æœæ£€æŸ¥
                if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                    logger.warning(f"æµ‹è¯•æ‰¹æ¬¡{batch_idx}: nDSMé¢„æµ‹åŒ…å«NaNæˆ–Inf")
                    failed_batches += 1
                    continue
                
                # ç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
                if predictions.shape != targets.shape:
                    if predictions.dim() == 3 and targets.dim() == 2:
                        targets = targets.unsqueeze(0) if targets.shape[0] != predictions.shape[0] else targets
                    elif predictions.shape[-2:] != targets.shape[-2:]:
                        predictions = F.interpolate(
                            predictions.unsqueeze(1) if predictions.dim() == 3 else predictions,
                            size=targets.shape[-2:],
                            mode='bilinear',
                            align_corners=False
                        )
                        if predictions.dim() == 4:
                            predictions = predictions.squeeze(1)
                
                # æ•°å€¼èŒƒå›´æ£€æŸ¥
                predictions = torch.clamp(predictions, 0, 1)
                targets = torch.clamp(targets, 0, 1)
                
                # è®¡ç®—æŸå¤±ï¼ˆè€ƒè™‘maskï¼‰
                if criterion is not None:
                    try:
                        # å¦‚æœæŸå¤±å‡½æ•°æ”¯æŒmask
                        if hasattr(criterion, 'forward') and 'masks' in criterion.forward.__code__.co_varnames:
                            loss = criterion(predictions, targets, masks)
                        else:
                            # æ‰‹åŠ¨åº”ç”¨mask
                            valid_mask = (masks > 0.5) & (targets >= 0)
                            if valid_mask.sum() > 0:
                                loss = criterion(predictions[valid_mask], targets[valid_mask])
                            else:
                                continue
                        
                        if not (torch.isnan(loss) or torch.isinf(loss)):
                            batch_losses.append(loss.item())
                    except Exception as e:
                        logger.warning(f"æµ‹è¯•æ‰¹æ¬¡{batch_idx}æŸå¤±è®¡ç®—é”™è¯¯: {e}")
                
                # æ›´æ–°åœ¨çº¿æŒ‡æ ‡ï¼ˆä¼ å…¥maskï¼‰
                metrics_calculator.update(
                    predictions.cpu().numpy(), 
                    targets.cpu().numpy(),
                    masks.cpu().numpy() if len(batch_data) == 3 else None
                )
                
                # æ›´æ–°è¿›åº¦æ¡
                current_metrics = metrics_calculator.compute_metrics()
                test_pbar.set_postfix({
                    'samples': current_metrics.get('total_samples', 0),
                    'mae': f"{current_metrics.get('mae', 0):.3f}",
                    'rmse': f"{current_metrics.get('rmse', 0):.3f}",
                    'r2': f"{current_metrics.get('r2', 0):.3f}"
                })
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if batch_idx % memory_cleanup_interval == 0 and batch_idx > 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
                
                # æ¸…ç†å½“å‰æ‰¹æ¬¡çš„å¼ é‡
                del images, targets, predictions
                if len(batch_data) == 3:
                    del masks
                
            except Exception as e:
                logger.error(f"æµ‹è¯•æ‰¹æ¬¡{batch_idx}å¤„ç†é”™è¯¯: {e}")
                failed_batches += 1
                continue
    
    total_time = time.time() - start_time
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_metrics = metrics_calculator.compute_metrics()
    
    if final_metrics.get('total_samples', 0) == 0:
        logger.error("æµ‹è¯•è¿‡ç¨‹ä¸­æ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆæ•°æ®!")
        return None
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œæ¨ç†æ—¶é—´
    avg_loss = np.mean(batch_losses) if batch_losses else float('inf')
    avg_inference_time = np.mean(inference_times) if inference_times else 0
    
    logger.info(f"æµ‹è¯•å®Œæˆ:")
    logger.info(f"  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.4f} ç§’/æ‰¹æ¬¡")
    logger.info(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
    logger.info(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {final_metrics['total_samples']}")
    logger.info(f"  å¤±è´¥æ‰¹æ¬¡æ•°: {failed_batches}")
    
    return {
        'metrics': final_metrics,
        'avg_loss': avg_loss,
        'avg_inference_time': avg_inference_time,
        'total_time': total_time,
        'batch_losses': batch_losses,
        'failed_batches': failed_batches
    }

def save_test_results(test_results, save_dir, logger, model_type):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    results_file = os.path.join(save_dir, f'test_results_{model_type}.json')
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'model_type': model_type,
        'metrics': test_results['metrics'],
        'performance': {
            'avg_loss': test_results['avg_loss'],
            'avg_inference_time': test_results['avg_inference_time'],
            'total_time': test_results['total_time'],
            'failed_batches': test_results['failed_batches']
        }
    }
    
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

def create_quick_visualizations(test_results, save_dir, logger, model_type):
    """åˆ›å»ºå¿«é€Ÿå¯è§†åŒ–ï¼ˆä»…ä½¿ç”¨é‡‡æ ·æ•°æ®ï¼‰"""
    logger.info("ç”Ÿæˆå¿«é€Ÿå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # ä»ç»“æœä¸­è·å–åŸºæœ¬ä¿¡æ¯
        metrics = test_results['metrics']
        
        # åˆ›å»ºæŒ‡æ ‡æ€»ç»“å›¾
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f'{model_type.upper()} æ¨¡å‹æµ‹è¯•ç»“æœ', fontsize=16)
        
        # 1. åŸºç¡€æŒ‡æ ‡æ¡å½¢å›¾
        ax = axes[0, 0]
        basic_metrics = ['mae', 'rmse', 'r2']
        basic_values = [metrics.get(m, 0) for m in basic_metrics]
        basic_labels = ['MAE (m)', 'RMSE (m)', 'RÂ²']
        
        bars = ax.bar(basic_labels, basic_values, color=['skyblue', 'lightcoral', 'lightgreen'])
        ax.set_title('åŸºç¡€è¯„ä¼°æŒ‡æ ‡')
        ax.set_ylabel('å€¼')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, basic_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
        
        # 2. ç²¾åº¦æŒ‡æ ‡
        ax = axes[0, 1]
        accuracy_metrics = ['height_accuracy_1m', 'height_accuracy_2m', 'height_accuracy_5m', 'height_accuracy_10m']
        accuracy_values = [metrics.get(m, 0) * 100 for m in accuracy_metrics]  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        accuracy_labels = ['Â±1m', 'Â±2m', 'Â±5m', 'Â±10m']
        
        bars = ax.bar(accuracy_labels, accuracy_values, color='orange', alpha=0.7)
        ax.set_title('é«˜åº¦ç²¾åº¦æŒ‡æ ‡')
        ax.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax.set_ylim(0, 100)
        
        for bar, value in zip(bars, accuracy_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{value:.1f}%', ha='center', va='bottom')
        
        # 3. åˆ†å±‚è¯¯å·®
        ax = axes[1, 0]
        layer_metrics = ['mae_ground_level', 'mae_low_buildings', 'mae_mid_buildings', 'mae_high_buildings']
        layer_values = [metrics.get(m, 0) for m in layer_metrics]
        layer_labels = ['åœ°é¢å±‚\n(-5~5m)', 'ä½å»ºç­‘\n(5~20m)', 'ä¸­å»ºç­‘\n(20~50m)', 'é«˜å»ºç­‘\n(>50m)']
        
        bars = ax.bar(layer_labels, layer_values, color='purple', alpha=0.7)
        ax.set_title('åˆ†å±‚è¯¯å·®åˆ†æ')
        ax.set_ylabel('MAE (m)')
        
        for bar, value in zip(bars, layer_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.2f}', ha='center', va='bottom')
        
        # 4. æ•°æ®èŒƒå›´å¯¹æ¯”
        ax = axes[1, 1]
        range_data = [
            [metrics.get('data_range_min', 0), metrics.get('data_range_max', 0)],
            [metrics.get('prediction_range_min', 0), metrics.get('prediction_range_max', 0)]
        ]
        range_labels = ['çœŸå®å€¼', 'é¢„æµ‹å€¼']
        
        x = np.arange(len(range_labels))
        width = 0.35
        
        ax.bar(x - width/2, [r[0] for r in range_data], width, label='æœ€å°å€¼', alpha=0.7)
        ax.bar(x + width/2, [r[1] for r in range_data], width, label='æœ€å¤§å€¼', alpha=0.7)
        
        ax.set_title('æ•°æ®èŒƒå›´å¯¹æ¯”')
        ax.set_ylabel('é«˜åº¦ (m)')
        ax.set_xticks(x)
        ax.set_xticklabels(range_labels)
        ax.legend()
        
        plt.tight_layout()
        
        summary_path = os.path.join(save_dir, f'test_summary_{model_type}.png')
        plt.savefig(summary_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {summary_path}")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬ï¼ˆå¤§æ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ï¼‰')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='/mnt/data1/UserData/hudong26/HeightData/',
                        help='æ•°æ®æ ¹ç›®å½• (åŒ…å«train/val/testå­ç›®å½•)')
    parser.add_argument('--stats_json_path', type=str, default='./gamus_full_stats.json',
                        help='é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°ï¼ˆä¼šä»æ£€æŸ¥ç‚¹è‡ªåŠ¨æ¨æ–­ï¼Œè¿™é‡Œä½œä¸ºå¤‡ç”¨ï¼‰
    parser.add_argument('--model_type', type=str, default='auto',
                        choices=['auto', 'gamus', 'depth2elevation'],
                        help='æ¨¡å‹ç±»å‹ï¼ˆautoè¡¨ç¤ºä»æ£€æŸ¥ç‚¹è‡ªåŠ¨æ¨æ–­ï¼‰')
    parser.add_argument('--encoder', type=str, default='vitb',
                        choices=['vits', 'vitb', 'vitl'],
                        help='ç¼–ç å™¨ç±»å‹ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰')
    parser.add_argument('--pretrained_path', type=str, default=None,
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºæ¨¡å‹ç»“æ„åˆ›å»ºï¼‰')
    
    # æ•°æ®å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    parser.add_argument('--normalization_method', type=str, default='minmax',
                        choices=['minmax', 'percentile', 'zscore'],
                        help='å½’ä¸€åŒ–æ–¹æ³•ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰')
    parser.add_argument('--min_height', type=float, default=-5.0,
                        help='æœ€å°é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--max_height', type=float, default=200.0,
                        help='æœ€å¤§é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    
    # maskç›¸å…³å‚æ•°
    parser.add_argument('--mask_dir', type=str, default='/mnt/data1/UserData/hudong26/HeightData/',
                        help='classes maskæ ¹ç›®å½•')
    parser.add_argument('--building_class_id', type=int, default=3,
                        help='å»ºç­‘ç±»åˆ«ID')
    parser.add_argument('--tree_class_id', type=int, default=6,
                        help='æ ‘æœ¨ç±»åˆ«ID')
    # parser.add_argument('--use_all_classes', action='store_true',
    #                     help='ä½¿ç”¨æ‰€æœ‰ç±»åˆ«è€Œä¸åªæ˜¯building+tree')
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=1,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--save_dir', type=str, default='./test_results',
                        help='æµ‹è¯•ç»“æœä¿å­˜ç›®å½•')
    
    # å¤§æ•°æ®é›†ä¼˜åŒ–å‚æ•°
    parser.add_argument('--max_test_samples', type=int, default=0,
                        help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆ0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰')
    parser.add_argument('--memory_cleanup_interval', type=int, default=50,
                        help='å†…å­˜æ¸…ç†é—´éš”ï¼ˆæ‰¹æ¬¡æ•°ï¼‰')
    parser.add_argument('--disable_pin_memory', action='store_true',
                        help='ç¦ç”¨pin_memoryä»¥èŠ‚çœå†…å­˜')
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--loss_type', type=str, default='mse',
                        choices=['mse', 'mae', 'huber', 'focal', 'combined'],
                        help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--height_aware_loss', action='store_true',
                        help='å¯ç”¨é«˜åº¦æ„ŸçŸ¥æŸå¤±æƒé‡')
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument('--enable_visualization', action='store_true',
                        help='å¯ç”¨ç»“æœå¯è§†åŒ–')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(args.save_dir, f'test_log_{timestamp}.log')
    logger = setup_logger(log_file)
    
    logger.info("=" * 80)
    logger.info("å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼ˆå¤§æ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    logger.info("=" * 80)
    logger.info(f"æµ‹è¯•å‚æ•°: {vars(args)}")
    
    try:
        # è®¾å¤‡è®¾ç½®
        if args.device == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            if torch.cuda.is_available():
                logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device(args.device)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # è®¾ç½®å†…å­˜ä¼˜åŒ–
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            logger.info("å¯ç”¨CUDAä¼˜åŒ–")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡å’Œæ¨¡å‹ç±»å‹
        model_state_dict, detected_model_type = load_trained_model(args.checkpoint_path, device, logger)
        
        # ç¡®å®šæ¨¡å‹ç±»å‹
        if args.model_type == 'auto':
            model_type = detected_model_type
        else:
            model_type = args.model_type
        
        logger.info(f"ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
        
        # åˆ›å»ºæ¨¡å‹ç»“æ„
        logger.info("åˆ›å»ºæ¨¡å‹ç»“æ„...")
        try:
            model_kwargs = {
                'encoder': args.encoder,
                'pretrained_path': args.pretrained_path,
                'freeze_encoder': True,  # æµ‹è¯•æ—¶å†»ç»“ç¼–ç å™¨
                'model_type': model_type
            }
            
            # ä¸ºDepth2Elevationæ·»åŠ ç‰¹å®šå‚æ•°
            if model_type == 'depth2elevation':
                model_kwargs.update({
                    'img_size': 448,
                    'patch_size': 14,
                    'use_multi_scale_output': False,  # æµ‹è¯•æ—¶ä½¿ç”¨å•å°ºåº¦
                    'loss_config': {},
                    'freezing_config': {}
                })
            
            model = create_gamus_model(**model_kwargs)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise
        
        # åŠ è½½æƒé‡
        model.load_state_dict(model_state_dict, strict=False)
        model = model.to(device)
        model.eval()
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_loader, test_dataset = create_test_dataset(args.data_dir, args, logger)
        
        # è·å–å½’ä¸€åŒ–å™¨ï¼ˆç”¨äºæŸå¤±å‡½æ•°ï¼‰
        if hasattr(test_dataset, 'get_normalizer'):
            height_normalizer = test_dataset.get_normalizer()
        else:
            # Subsetæƒ…å†µï¼Œä»åŸå§‹æ•°æ®é›†è·å–
            height_normalizer = test_dataset.dataset.get_normalizer()
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        criterion = create_height_loss(
            loss_type=args.loss_type,
            height_aware=args.height_aware_loss,
            height_normalizer=height_normalizer,
            min_height=args.min_height,
            max_height=args.max_height
        )
        
        # æ‰§è¡Œä¼˜åŒ–çš„æµ‹è¯•
        test_results = test_model_optimized(
            model, test_loader, device, logger, criterion, 
            args.memory_cleanup_interval
        )
        
        if test_results is None:
            logger.error("æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è·å¾—æœ‰æ•ˆç»“æœ")
            return 1
        
        metrics = test_results['metrics']
        
        # è¾“å‡ºè¯¦ç»†ç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info(f"{model_type.upper()} æ¨¡å‹æµ‹è¯•ç»“æœ")
        logger.info("=" * 60)
        logger.info(f"åŸºç¡€æŒ‡æ ‡:")
        logger.info(f"  MAE: {metrics['mae']:.4f} m")
        logger.info(f"  RMSE: {metrics['rmse']:.4f} m")
        logger.info(f"  RÂ²: {metrics['r2']:.4f}")
        logger.info(f"  Pearson r: {metrics['pearson_r']:.4f}")
        
        logger.info(f"\nnDSMé«˜åº¦ç²¾åº¦:")
        logger.info(f"  Â±1m: {metrics['height_accuracy_1m']:.1%}")
        logger.info(f"  Â±2m: {metrics['height_accuracy_2m']:.1%}")
        logger.info(f"  Â±5m: {metrics['height_accuracy_5m']:.1%}")
        logger.info(f"  Â±10m: {metrics['height_accuracy_10m']:.1%}")
        
        logger.info(f"\nåˆ†å±‚è¯¯å·®åˆ†æ:")
        logger.info(f"  åœ°é¢å±‚(-5~5m): {metrics['mae_ground_level']:.2f} m")
        logger.info(f"  ä½å»ºç­‘(5~20m): {metrics['mae_low_buildings']:.2f} m")
        logger.info(f"  ä¸­å»ºç­‘(20~50m): {metrics['mae_mid_buildings']:.2f} m")
        logger.info(f"  é«˜å»ºç­‘(>50m): {metrics['mae_high_buildings']:.2f} m")
        
        logger.info(f"\næ•°æ®èŒƒå›´:")
        logger.info(f"  çœŸå®å€¼èŒƒå›´: [{metrics['data_range_min']:.2f}, {metrics['data_range_max']:.2f}] m")
        logger.info(f"  é¢„æµ‹å€¼èŒƒå›´: [{metrics['prediction_range_min']:.2f}, {metrics['prediction_range_max']:.2f}] m")
        
        logger.info(f"\næ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"  å¹³å‡æ¨ç†æ—¶é—´: {test_results['avg_inference_time']:.4f} ç§’/æ‰¹æ¬¡")
        logger.info(f"  æ€»æµ‹è¯•æ—¶é—´: {test_results['total_time']:.2f} ç§’")
        logger.info(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {metrics['total_samples']:,}")
        logger.info(f"  å¤±è´¥æ‰¹æ¬¡æ•°: {test_results['failed_batches']}")
        
        # è®¡ç®—æ¯ç§’å¤„ç†æ ·æœ¬æ•°
        samples_per_second = metrics['total_samples'] / test_results['total_time']
        logger.info(f"  å¤„ç†é€Ÿåº¦: {samples_per_second:.1f} æ ·æœ¬/ç§’")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        save_test_results(test_results, args.save_dir, logger, model_type)
        
        # ç”Ÿæˆå¿«é€Ÿå¯è§†åŒ–
        if args.enable_visualization:
            create_quick_visualizations(test_results, args.save_dir, logger, model_type)
        
        logger.info("=" * 60)
        logger.info("æµ‹è¯•å®Œæˆ!")
        logger.info(f"ç»“æœä¿å­˜åœ¨: {args.save_dir}")
        
        # æ€§èƒ½æ€»ç»“
        logger.info(f"\n{model_type.upper()} æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        logger.info(f"  æœ€ä½³æŒ‡æ ‡: MAE={metrics['mae']:.3f}m, RMSE={metrics['rmse']:.3f}m, RÂ²={metrics['r2']:.3f}")
        logger.info(f"  æ¨ç†æ•ˆç‡: {samples_per_second:.1f} æ ·æœ¬/ç§’")
        
        if metrics['mae'] < 2.0:
            logger.info("  ğŸ‰ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ (MAE < 2.0m)")
        elif metrics['mae'] < 5.0:
            logger.info("  âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ (MAE < 5.0m)")
        else:
            logger.info("  âš ï¸  æ¨¡å‹æ€§èƒ½æœ‰å¾…æ”¹è¿› (MAE >= 5.0m)")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    exit_code = main()
    exit(exit_code)