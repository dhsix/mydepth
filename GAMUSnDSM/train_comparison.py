#!/usr/bin/env python3
"""
æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”çš„GAMUS nDSMè®­ç»ƒè„šæœ¬
å¯ä»¥è®­ç»ƒGAMUSæ¨¡å‹å’ŒDepth2Elevationæ¨¡å‹
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
import logging
import time
import argparse
import warnings
from datetime import datetime
from tqdm import tqdm
import json
from sklearn.metrics import mean_squared_error, mean_absolute_error

warnings.filterwarnings('ignore')

# å¯¼å…¥ä¿®æ”¹åçš„æ•°æ®é›†æ¨¡å—
from improved_dataset_with_mask import create_gamus_dataloader
from improved_normalization_loss import create_height_loss
from model_with_comparison import create_gamus_model  # ä½¿ç”¨æ”¯æŒå¤šæ¨¡å‹çš„ç‰ˆæœ¬

def setup_logger(log_path):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logger = logging.getLogger('gamus_training')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    console_handler = logging.StreamHandler()
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def create_datasets(args, logger):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
    logger.info("åˆ›å»ºæ•°æ®é›†...")
    
    # æ„å»ºæ•°æ®è·¯å¾„
    train_image_dir = os.path.join(args.data_dir, 'train','images' )
    train_label_dir = os.path.join(args.data_dir, 'train','depths' )
    val_image_dir = os.path.join(args.data_dir, 'val','images' )
    val_label_dir = os.path.join(args.data_dir, 'val','depths' )
    
    # maskè·¯å¾„
    train_mask_dir = None
    val_mask_dir = None
    if args.mask_dir:
        train_mask_dir = os.path.join(args.mask_dir, 'train', 'classes')
        val_mask_dir = os.path.join(args.mask_dir, 'val', 'classes')
        
        logger.info(f"æ£€æŸ¥maskæ•°æ®è·¯å¾„:")
        logger.info(f"  è®­ç»ƒmaskç›®å½•: {train_mask_dir}")
        logger.info(f"  éªŒè¯maskç›®å½•: {val_mask_dir}")
    
    logger.info(f"æ£€æŸ¥è®­ç»ƒæ•°æ®è·¯å¾„:")
    logger.info(f"  å›¾åƒç›®å½•: {train_image_dir}")
    logger.info(f"  æ ‡ç­¾ç›®å½•: {train_label_dir}")
    
    # éªŒè¯è·¯å¾„
    required_paths = [train_image_dir, train_label_dir]
    if train_mask_dir:
        required_paths.append(train_mask_dir)
        
    for path in required_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {path}")
        else:
            files = os.listdir(path)
            logger.info(f"  {path}: {len(files)} ä¸ªæ–‡ä»¶")
    
    # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶è·¯å¾„
    if not args.stats_json_path:
        args.stats_json_path = os.path.join(args.save_dir, 'gamus_stats.json')
        logger.warning(f"âš ï¸ æœªæŒ‡å®šç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {args.stats_json_path}")
    
    if not os.path.exists(args.stats_json_path):
        logger.error(f"âŒ ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {args.stats_json_path}")
        logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python precompute_stats.py <data_dir> --output <stats_file>")
        raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œé¢„è®¡ç®—è„šæœ¬ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶")    
    
    # è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨
    height_filter = {
        'min_height': args.min_height,
        'max_height': args.max_height
    }
    logger.info(f"é«˜åº¦è¿‡æ»¤å™¨: {height_filter}")
    
    # maské…ç½®
    if args.mask_dir:
        logger.info(f"ğŸ¯ Maské…ç½®:")
        logger.info(f"   å»ºç­‘ç±»åˆ«ID: {args.building_class_id}")
        logger.info(f"   æ ‘æœ¨ç±»åˆ«ID: {args.tree_class_id}")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    logger.info("æ­£åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    try:
        train_loader, train_dataset = create_gamus_dataloader(
            image_dir=train_image_dir,
            label_dir=train_label_dir,
            mask_dir=train_mask_dir,
            building_class_id=args.building_class_id,
            tree_class_id=args.tree_class_id,
            batch_size=args.batch_size,
            shuffle=True,
            normalization_method=args.normalization_method,
            enable_augmentation=args.enable_augmentation,
            stats_json_path=args.stats_json_path,
            height_filter=height_filter,
            force_recompute=False,
            num_workers=min(args.num_workers, 2)
        )
        
        if hasattr(train_dataset, 'global_min'):
            logger.info(f"âœ… ä½¿ç”¨é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œæ•°æ®èŒƒå›´: [{train_dataset.global_min:.2f}, {train_dataset.global_max:.2f}] ç±³")
        
        # è·å–é«˜åº¦å½’ä¸€åŒ–å™¨
        height_normalizer = train_dataset.get_normalizer()
        if hasattr(height_normalizer, 'global_min_h') and hasattr(height_normalizer, 'global_max_h'):
            min_height = height_normalizer.global_min_h
            max_height = height_normalizer.global_max_h
            logger.info(f"âœ… ä»å½’ä¸€åŒ–å™¨è·å–çœŸå®é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        elif hasattr(height_normalizer, 'min_val') and hasattr(height_normalizer, 'max_val'):
            min_height = height_normalizer.min_val
            max_height = height_normalizer.max_val
            logger.info(f"âœ… ä»å½’ä¸€åŒ–å™¨è·å–çœŸå®é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        else:
            min_height = args.min_height
            max_height = args.max_height
            logger.warning(f"âš ï¸ æ— æ³•ä»å½’ä¸€åŒ–å™¨è·å–é«˜åº¦èŒƒå›´ï¼Œä½¿ç”¨é…ç½®å‚æ•°: [{min_height:.2f}, {max_height:.2f}] ç±³")
        
        height_range = max_height - min_height
        logger.info(f"ğŸ“Š é«˜åº¦èŒƒå›´è·¨åº¦: {height_range:.2f} ç±³")
        
        logger.info("âœ“ è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"åˆ›å»ºè®­ç»ƒæ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    val_loader = None
    if os.path.exists(val_image_dir) and os.path.exists(val_label_dir):
        if args.mask_dir and not os.path.exists(val_mask_dir):
            logger.warning(f"éªŒè¯é›†maskç›®å½•ä¸å­˜åœ¨: {val_mask_dir}ï¼Œå°†ä¸ä½¿ç”¨mask")
            val_mask_dir = None
            
        logger.info("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        try:
            val_loader, _ = create_gamus_dataloader(
                image_dir=val_image_dir,
                label_dir=val_label_dir,
                mask_dir=val_mask_dir,
                building_class_id=args.building_class_id,
                tree_class_id=args.tree_class_id,
                batch_size=args.batch_size,
                shuffle=False,
                normalization_method=args.normalization_method,
                enable_augmentation=False,
                stats_json_path=args.stats_json_path,
                height_filter=height_filter,
                force_recompute=False,
                num_workers=min(args.num_workers, 2)
            )
            logger.info(f"âœ“ éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(val_loader.dataset)} ä¸ªæ ·æœ¬")
        except Exception as e:
            logger.error(f"åˆ›å»ºéªŒè¯æ•°æ®é›†å¤±è´¥: {e}")
            val_loader = None
    else:
        logger.warning("æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†è¿›è¡ŒéªŒè¯")
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    logger.info(f"æ•°æ®èŒƒå›´: [{height_filter['min_height']}, {height_filter['max_height']}] ç±³")
    if args.mask_dir:
        logger.info(f"ä½¿ç”¨mask: ä»…è®­ç»ƒbuildingå’ŒtreeåŒºåŸŸ")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    logger.info("æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        test_batch = next(iter(train_loader))
        if len(test_batch) == 3:
            images, labels, masks = test_batch
            logger.info(f"âœ“ æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸ: å›¾åƒ {images.shape}, æ ‡ç­¾ {labels.shape}, mask {masks.shape}")
            logger.info(f"  maskç»Ÿè®¡: min={masks.min():.3f}, max={masks.max():.3f}, mean={masks.mean():.3f}")
        else:
            images, labels = test_batch
            logger.info(f"âœ“ æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸ: å›¾åƒ {images.shape}, æ ‡ç­¾ {labels.shape}")
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        raise
    
    return train_loader, val_loader, train_dataset, height_normalizer, min_height, max_height

class SimpleGAMUSValidator:
    """ç®€åŒ–çš„GAMUSéªŒè¯å™¨ï¼ˆæ”¯æŒmaskï¼‰"""
    
    def __init__(self, height_normalizer, logger=None):
        self.height_normalizer = height_normalizer
        self.logger = logger or logging.getLogger(__name__)
        
    def denormalize_height(self, normalized_data):
        """ä½¿ç”¨å½’ä¸€åŒ–å™¨å°†å½’ä¸€åŒ–çš„nDSMæ•°æ®è¿˜åŸåˆ°çœŸå®é«˜åº¦å€¼"""
        return self.height_normalizer.denormalize(normalized_data)
    
    def validate_with_metrics(self, model, val_loader, criterion, device, epoch=None):
        """æ‰§è¡ŒéªŒè¯å¹¶è¿”å›è¯¦ç»†æŒ‡æ ‡ï¼ˆæ”¯æŒmaskï¼‰"""
        model.eval()
        
        total_loss = 0.0
        total_count = 0
        
        all_preds_real = []
        all_targets_real = []
        max_samples = 100000
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f'Validation', leave=False)
            
            for batch_idx, batch_data in enumerate(pbar):
                if len(batch_data) == 3:
                    images, labels, masks = batch_data
                    images = images.to(device)
                    labels = labels.to(device)
                    masks = masks.to(device)
                else:
                    images, labels = batch_data
                    images = images.to(device)
                    labels = labels.to(device)
                    masks = torch.ones_like(labels).to(device)
                
                try:
                    predictions = model(images)
                    
                    if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                        self.logger.warning(f"é¢„æµ‹å€¼åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                        continue
                    
                    # è®¡ç®—å¸¦maskçš„æŸå¤±
                    if isinstance(criterion, MaskedLoss):
                        loss = criterion(predictions, labels, masks)
                    else:
                        valid_mask = (masks > 0.5) & (labels >= 0)
                        if valid_mask.sum() > 0:
                            loss = criterion(predictions[valid_mask], labels[valid_mask])
                        else:
                            continue
                    
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.warning(f"æŸå¤±å€¼æ— æ•ˆï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                        continue
                    
                    total_loss += loss.item()
                    total_count += 1
                    
                    # æ”¶é›†æ ·æœ¬ç”¨äºæŒ‡æ ‡è®¡ç®—
                    if len(all_preds_real) < max_samples:
                        valid_mask = (masks > 0.5) & (labels >= 0)
                        if valid_mask.sum() > 0:
                            preds_cpu = predictions[valid_mask].detach().cpu().numpy().flatten()
                            targets_cpu = labels[valid_mask].detach().cpu().numpy().flatten()
                            
                            n_samples = min(500, len(preds_cpu))
                            if len(preds_cpu) > n_samples:
                                indices = np.random.choice(len(preds_cpu), n_samples, replace=False)
                                preds_cpu = preds_cpu[indices]
                                targets_cpu = targets_cpu[indices]
                            
                            preds_real = self.denormalize_height(preds_cpu)
                            targets_real = self.denormalize_height(targets_cpu)
                            
                            all_preds_real.extend(preds_real)
                            all_targets_real.extend(targets_real)
                    
                    pbar.set_postfix({'loss': f'{loss.item():.6f}'})
                    
                except Exception as e:
                    self.logger.warning(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} é”™è¯¯: {e}")
                    continue
        
        avg_loss = total_loss / total_count if total_count > 0 else float('inf')
        metrics = {'loss': avg_loss}
        
        if all_preds_real and all_targets_real:
            all_preds_real = np.array(all_preds_real)
            all_targets_real = np.array(all_targets_real)
            
            valid_mask = (~np.isnan(all_preds_real) & ~np.isnan(all_targets_real) & 
                         ~np.isinf(all_preds_real) & ~np.isinf(all_targets_real))
            
            if np.sum(valid_mask) > 10:
                valid_preds = all_preds_real[valid_mask]
                valid_targets = all_targets_real[valid_mask]
                
                mae = mean_absolute_error(valid_targets, valid_preds)
                mse = mean_squared_error(valid_targets, valid_preds)
                rmse = np.sqrt(mse)
                
                ss_res = np.sum((valid_targets - valid_preds) ** 2)
                ss_tot = np.sum((valid_targets - np.mean(valid_targets)) ** 2)
                r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-10 else 0.0
                
                errors = np.abs(valid_preds - valid_targets)
                accuracy_1m = np.mean(errors <= 1.0)
                accuracy_2m = np.mean(errors <= 2.0)
                accuracy_5m = np.mean(errors <= 5.0)
                
                ground_mask = (valid_targets >= -5) & (valid_targets <= 5)
                low_mask = (valid_targets > 5) & (valid_targets <= 20)
                mid_mask = (valid_targets > 20) & (valid_targets <= 50)
                high_mask = valid_targets > 50
                
                mae_ground = np.mean(errors[ground_mask]) if np.sum(ground_mask) > 0 else 0.0
                mae_low = np.mean(errors[low_mask]) if np.sum(low_mask) > 0 else 0.0
                mae_mid = np.mean(errors[mid_mask]) if np.sum(mid_mask) > 0 else 0.0
                mae_high = np.mean(errors[high_mask]) if np.sum(high_mask) > 0 else 0.0
                
                metrics.update({
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'accuracy_1m': accuracy_1m,
                    'accuracy_2m': accuracy_2m,
                    'accuracy_5m': accuracy_5m,
                    'mae_ground': mae_ground,
                    'mae_low': mae_low,
                    'mae_mid': mae_mid,
                    'mae_high': mae_high,
                    'valid_samples': len(valid_preds),
                    'data_range': f'[{valid_targets.min():.1f}, {valid_targets.max():.1f}]m'
                })
        
        return metrics
    
    def log_metrics(self, epoch, metrics, is_best=False):
        """è®°å½•æŒ‡æ ‡"""
        self.logger.info(f'éªŒè¯æŒ‡æ ‡ - Epoch {epoch}:')
        self.logger.info(f'  æŸå¤±: {metrics.get("loss", "N/A"):.6f}')
        if 'mae' in metrics:
            self.logger.info(f'  MAE: {metrics["mae"]:.4f} m')
            self.logger.info(f'  RMSE: {metrics["rmse"]:.4f} m')
            self.logger.info(f'  RÂ²: {metrics["r2"]:.4f}')
            self.logger.info(f'  ç²¾åº¦: Â±1m={metrics["accuracy_1m"]:.1%}, Â±2m={metrics["accuracy_2m"]:.1%}, Â±5m={metrics["accuracy_5m"]:.1%}')
            self.logger.info(f'  åˆ†å±‚MAE: åœ°é¢={metrics["mae_ground"]:.2f}m, ä½å»ºç­‘={metrics["mae_low"]:.2f}m, ä¸­å»ºç­‘={metrics["mae_mid"]:.2f}m, é«˜å»ºç­‘={metrics["mae_high"]:.2f}m')
            self.logger.info(f'  æ•°æ®èŒƒå›´: {metrics["data_range"]}, æœ‰æ•ˆæ ·æœ¬: {metrics["valid_samples"]}')
        
        if is_best:
            self.logger.info('  â˜… æœ€ä½³éªŒè¯æ€§èƒ½ â˜…')

class MaskedLoss(nn.Module):
    """å¸¦maskçš„æŸå¤±å‡½æ•°åŒ…è£…å™¨"""
    
    def __init__(self, base_criterion):
        super().__init__()
        self.base_criterion = base_criterion
    
    def forward(self, predictions, targets, masks):
        valid_mask = (masks > 0.5) & (targets >= 0)
        
        if valid_mask.sum() == 0:
            return torch.tensor(0.0, device=predictions.device, requires_grad=True)
        
        valid_preds = predictions[valid_mask]
        valid_targets = targets[valid_mask]
        
        return self.base_criterion(valid_preds, valid_targets)

def validate_model_enhanced(model, val_loader, criterion, device, logger, height_normalizer, epoch=None):
    """å¢å¼ºçš„éªŒè¯å‡½æ•°"""
    if val_loader is None:
        return {'loss': 0.0, 'count': 0}
    
    validator = SimpleGAMUSValidator(height_normalizer, logger)
    metrics = validator.validate_with_metrics(model, val_loader, criterion, device, epoch)
    validator.log_metrics(epoch, metrics)
    
    return metrics

def train_epoch(model, train_loader, criterion, optimizer, device, logger, epoch):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒmaskï¼‰"""
    model.train()
    total_loss = 0.0
    total_count = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    
    for batch_idx, batch_data in enumerate(pbar):
        if len(batch_data) == 3:
            images, labels, masks = batch_data
            images = images.to(device)
            labels = labels.to(device)
            masks = masks.to(device)
        else:
            images, labels = batch_data
            images = images.to(device)
            labels = labels.to(device)
            masks = torch.ones_like(labels).to(device)
        
        try:
            optimizer.zero_grad()
            
            predictions = model(images)
            
            if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                logger.warning(f"é¢„æµ‹å€¼åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                continue
            
            # è®¡ç®—å¸¦maskçš„æŸå¤±
            if isinstance(criterion, MaskedLoss):
                loss = criterion(predictions, labels, masks)
            else:
                valid_mask = (masks > 0.5) & (labels >= 0)
                if valid_mask.sum() == 0:
                    continue
                loss = criterion(predictions[valid_mask], labels[valid_mask])
            
            if torch.isnan(loss) or torch.isinf(loss) or loss > 10:
                logger.warning(f"å¼‚å¸¸æŸå¤±å€¼ {loss.item():.6f}ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                continue
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            total_count += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.6f}',
                'avg_loss': f'{total_loss/total_count:.6f}'
            })
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error(f"GPUå†…å­˜ä¸è¶³: {e}")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
            else:
                logger.error(f"è®­ç»ƒé”™è¯¯: {e}")
                continue
        except Exception as e:
            logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
            continue
    
    avg_loss = total_loss / total_count if total_count > 0 else float('inf')
    logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    return avg_loss

def save_checkpoint(epoch, model, optimizer, loss, save_dir, is_best=False, model_type='gamus'):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'model_type': model_type,  # æ–°å¢ï¼šè®°å½•æ¨¡å‹ç±»å‹
        'timestamp': datetime.now().isoformat()
    }
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æ–‡ä»¶å
    prefix = f'{model_type}_'
    
    latest_path = os.path.join(save_dir, f'{prefix}latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)
    
    if is_best:
        best_path = os.path.join(save_dir, f'{prefix}best_model.pth')
        torch.save(checkpoint, best_path)
        return best_path
    
    if epoch % 10 == 0:
        epoch_path = os.path.join(save_dir, f'{prefix}checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, epoch_path)
    
    return latest_path

def main():
    parser = argparse.ArgumentParser(description='æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”çš„GAMUS nDSMè®­ç»ƒè„šæœ¬')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--data_dir', type=str, default='/home/hudong26/HeightData/GAMUS/',
                        help='æ•°æ®æ ¹ç›®å½• (åŒ…å«train/valå­ç›®å½•)')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--stats_json_path', type=str, default='./gamus_full_stats.json',
                        help='é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„')
    
    # æ–°å¢ï¼šæ¨¡å‹é€‰æ‹©å‚æ•°
    parser.add_argument('--model_type', type=str, default='gamus',
                        choices=['gamus', 'depth2elevation'],
                        help='æ¨¡å‹ç±»å‹é€‰æ‹©')
    
    # maskç›¸å…³å‚æ•°
    parser.add_argument('--mask_dir', type=str, default=None,
                        help='classes maskæ ¹ç›®å½• (åŒ…å«train/val/classeså­ç›®å½•)')
    parser.add_argument('--building_class_id', type=int, default=6,
                        help='å»ºç­‘ç±»åˆ«ID')
    parser.add_argument('--tree_class_id', type=int, default=5,
                        help='æ ‘æœ¨ç±»åˆ«ID')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=40,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-5,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--num_workers', type=int, default=1,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--normalization_method', type=str, default='minmax',
                        choices=['minmax', 'percentile', 'zscore'],
                        help='å½’ä¸€åŒ–æ–¹æ³•')
    parser.add_argument('--min_height', type=float, default=-5.0,
                        help='æœ€å°é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--max_height', type=float, default=200.0,
                        help='æœ€å¤§é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--enable_augmentation', action='store_true',
                        help='å¯ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--force_recompute_stats', action='store_true',
                        help='å¼ºåˆ¶é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--debug', action='store_true',
                        help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--encoder', type=str, default='vitb',
                        choices=['vits', 'vitb', 'vitl'],
                        help='ç¼–ç å™¨ç±»å‹')
    parser.add_argument('--pretrained_path', type=str, 
                        default='/home/hudong26/hudong26/Height/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth',
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--freeze_encoder', action='store_true',
                        help='å†»ç»“ç¼–ç å™¨')
    
    # æ–°å¢ï¼šDepth2Elevationç‰¹å®šå‚æ•°
    parser.add_argument('--use_multi_scale_output', action='store_true',
                        help='ä½¿ç”¨å¤šå°ºåº¦è¾“å‡ºï¼ˆä»…å¯¹Depth2Elevationæœ‰æ•ˆï¼‰')
    parser.add_argument('--img_size', type=int, default=448,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--patch_size', type=int, default=14,
                        help='Patchå°ºå¯¸ï¼ˆä»…å¯¹Depth2Elevationæœ‰æ•ˆï¼‰')
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--loss_type', type=str, default='mse',
                        choices=['mse', 'mae', 'huber', 'focal', 'combined'],
                        help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--height_aware', action='store_true',
                        help='å¯ç”¨é«˜åº¦æ„ŸçŸ¥æŸå¤±')
    
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument('--patience', type=int, default=10,
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼')
    parser.add_argument('--early_stopping_patience', type=int, default=5,
                        help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--val_interval', type=int, default=1,
                        help='éªŒè¯é—´éš”')
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default='auto',
                        help='è®­ç»ƒè®¾å¤‡')
    
    args = parser.parse_args()
    
    # éªŒè¯å…³é”®å‚æ•°
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return 1
    
    if args.mask_dir and not os.path.exists(args.mask_dir):
        print(f"âŒ Maskç›®å½•ä¸å­˜åœ¨: {args.mask_dir}")
        return 1
    
    if args.stats_json_path and not os.path.exists(args.stats_json_path):
        print(f"âŒ ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {args.stats_json_path}")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python precompute_stats.py {args.data_dir} --output {args.stats_json_path}")
        return 1    
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(args.save_dir, f'training_{args.model_type}_{timestamp}.log')
    logger = setup_logger(log_file)
    
    if args.debug:
        logger.setLevel(logging.DEBUG)
    
    # æ‰“å°é…ç½®
    logger.info("=" * 60)
    logger.info(f"å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ - å½“å‰æ¨¡å‹: {args.model_type.upper()}")
    logger.info("=" * 60)
    logger.info(f"é…ç½®å‚æ•°: {json.dumps(vars(args), indent=2, ensure_ascii=False)}")
    
    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        logger.info(f"GPUä¿¡æ¯: {torch.cuda.get_device_name()}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        train_loader, val_loader, train_dataset, height_normalizer, min_height, max_height = create_datasets(args, logger)
        
        # åˆ›å»ºæ¨¡å‹ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹
        logger.info(f"åˆ›å»º{args.model_type}æ¨¡å‹...")
        
        # å‡†å¤‡æ¨¡å‹é…ç½®
        model_kwargs = {
            'encoder': args.encoder,
            'pretrained_path': args.pretrained_path,
            'freeze_encoder': args.freeze_encoder,
            'model_type': args.model_type
        }
        
        # ä¸ºDepth2Elevationæ·»åŠ ç‰¹å®šå‚æ•°
        if args.model_type == 'depth2elevation':
            model_kwargs.update({
                'img_size': args.img_size,
                'patch_size': args.patch_size,
                'use_multi_scale_output': args.use_multi_scale_output,
                'loss_config': {},  # å¯ä»¥æ ¹æ®éœ€è¦é…ç½®
                'freezing_config': {}  # å¯ä»¥æ ¹æ®éœ€è¦é…ç½®
            })
        
        model = create_gamus_model(**model_kwargs).to(device)
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"æ¨¡å‹å‚æ•°: æ€»æ•°={total_params:,}, å¯è®­ç»ƒ={trainable_params:,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=args.learning_rate,
            weight_decay=1e-4
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=args.patience,
            min_lr=1e-6,
            verbose=True
        )
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        base_criterion = create_height_loss(
            loss_type=args.loss_type,
            height_aware=args.height_aware,
            height_normalizer=height_normalizer,
            min_height=min_height,
            max_height=max_height
        )
        
        # åŒ…è£…ä¸ºå¸¦maskçš„æŸå¤±å‡½æ•°
        criterion = MaskedLoss(base_criterion)
        
        logger.info(f"ğŸ“Š æŸå¤±å‡½æ•°é…ç½®:")
        logger.info(f"   ç±»å‹: {args.loss_type} (å¸¦mask)")
        logger.info(f"   é«˜åº¦æ„ŸçŸ¥: {args.height_aware}")
        logger.info(f"   é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        if args.mask_dir:
            logger.info(f"   ä»…è®­ç»ƒ: building (ID={args.building_class_id}) å’Œ tree (ID={args.tree_class_id}) åŒºåŸŸ")
        
        # è®­ç»ƒå¾ªç¯
        logger.info("å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        start_time = time.time()
        
        for epoch in range(1, args.num_epochs + 1):
            # è®­ç»ƒ
            train_loss = train_epoch(
                model, train_loader, criterion, optimizer, device, logger, epoch
            )
            
            # éªŒè¯
            if epoch % args.val_interval == 0:
                val_metrics = validate_model_enhanced(
                    model, val_loader, criterion, device, logger, height_normalizer, epoch
                )
                val_loss = val_metrics['loss']
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_loss)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                saved_path = save_checkpoint(
                    epoch, model, optimizer, val_loss, args.save_dir, is_best, args.model_type
                )
                
                # æ‰“å°ç»“æœ
                logger.info(f"Epoch {epoch}/{args.num_epochs} ç»“æœ:")
                logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
                logger.info(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
                logger.info(f"  {'ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹!' if is_best else ''}")
                logger.info(f"  ä¿å­˜è·¯å¾„: {saved_path}")
                logger.info("-" * 60)
                
                # æ—©åœæ£€æŸ¥
                if patience_counter >= args.early_stopping_patience:
                    logger.info(f"æ—©åœè§¦å‘ (patience: {patience_counter})")
                    break
            
            # æ¸…ç†GPUå†…å­˜
            if device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        logger.info(f"è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ¨¡å‹ç±»å‹: {args.model_type}")
        logger.info(f"æ€»è€—æ—¶: {total_time / 3600:.2f} å°æ—¶")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"æ¨¡å‹ä¿å­˜åœ¨: {args.save_dir}")
        
    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        logger.info("è®­ç»ƒè„šæœ¬å·²é€€å‡º")

if __name__ == '__main__':
    main()