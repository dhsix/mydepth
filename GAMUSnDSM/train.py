#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆGAMUS nDSMè®­ç»ƒè„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼Œç®€åŒ–ç›‘æ§å’Œç®¡ç†åŠŸèƒ½
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
import logging
import time
import argparse
import warnings
from datetime import datetime
from tqdm import tqdm
import json
from sklearn.metrics import mean_squared_error, mean_absolute_error

warnings.filterwarnings('ignore')

# å¯¼å…¥ç®€åŒ–çš„æ¨¡å—
from improved_dataset import create_gamus_dataloader
from improved_normalization_loss import create_height_loss
from model import create_gamus_model

def setup_logger(log_path):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logger = logging.getLogger('gamus_training')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    console_handler = logging.StreamHandler()
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def create_datasets(args, logger):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
    logger.info("åˆ›å»ºæ•°æ®é›†...")
    
    # æ„å»ºæ•°æ®è·¯å¾„
    train_image_dir = os.path.join(args.data_dir, 'train','images' )
    train_label_dir = os.path.join(args.data_dir, 'train','depths' )
    val_image_dir = os.path.join(args.data_dir, 'val','images' )
    val_label_dir = os.path.join(args.data_dir, 'val','depths' )
    
    logger.info(f"æ£€æŸ¥è®­ç»ƒæ•°æ®è·¯å¾„:")
    logger.info(f"  å›¾åƒç›®å½•: {train_image_dir}")
    logger.info(f"  æ ‡ç­¾ç›®å½•: {train_label_dir}")
    
    # éªŒè¯è·¯å¾„
    for path in [train_image_dir, train_label_dir]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {path}")
        else:
            # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
            files = os.listdir(path)
            logger.info(f"  {path}: {len(files)} ä¸ªæ–‡ä»¶")
    # âœ… æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶è·¯å¾„
    if not args.stats_json_path:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        args.stats_json_path = os.path.join(args.save_dir, 'gamus_stats.json')
        logger.warning(f"âš ï¸ æœªæŒ‡å®šç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {args.stats_json_path}")
    
    if not os.path.exists(args.stats_json_path):
        logger.error(f"âŒ ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {args.stats_json_path}")
        logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python precompute_stats.py <data_dir> --output <stats_file>")
        raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œé¢„è®¡ç®—è„šæœ¬ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶")    
    
    # è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨
    height_filter = {
        'min_height': args.min_height,
        'max_height': args.max_height
    }
    logger.info(f"é«˜åº¦è¿‡æ»¤å™¨: {height_filter}")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    logger.info("æ­£åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    try:
        train_loader, train_dataset = create_gamus_dataloader(
            image_dir=train_image_dir,
            label_dir=train_label_dir,
            batch_size=args.batch_size,
            shuffle=True,
            normalization_method=args.normalization_method,
            enable_augmentation=args.enable_augmentation,
            stats_json_path=args.stats_json_path,
            height_filter=height_filter,
            force_recompute=False,
            num_workers=min(args.num_workers, 2)  # å‡å°‘workeræ•°é‡é¿å…å¡ä½
        )
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if hasattr(train_dataset, 'global_min'):
            logger.info(f"âœ… ä½¿ç”¨é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œæ•°æ®èŒƒå›´: [{train_dataset.global_min:.2f}, {train_dataset.global_max:.2f}] ç±³")
        # è·å–é«˜åº¦å½’ä¸€åŒ–å™¨
        height_normalizer = train_dataset.get_normalizer() # å‡è®¾æ•°æ®é›†æœ‰è¿™ä¸ªå±æ€§
        if hasattr(height_normalizer, 'global_min_h') and hasattr(height_normalizer, 'global_max_h'):
            min_height = height_normalizer.global_min_h
            max_height = height_normalizer.global_max_h
            logger.info(f"âœ… ä»å½’ä¸€åŒ–å™¨è·å–çœŸå®é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        elif hasattr(height_normalizer, 'min_val') and hasattr(height_normalizer, 'max_val'):
            min_height = height_normalizer.min_val
            max_height = height_normalizer.max_val
            logger.info(f"âœ… ä»å½’ä¸€åŒ–å™¨è·å–çœŸå®é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é…ç½®å‚æ•°
            min_height = args.min_height
            max_height = args.max_height
            logger.warning(f"âš ï¸ æ— æ³•ä»å½’ä¸€åŒ–å™¨è·å–é«˜åº¦èŒƒå›´ï¼Œä½¿ç”¨é…ç½®å‚æ•°: [{min_height:.2f}, {max_height:.2f}] ç±³")
        
        # âœ… è®¡ç®—é«˜åº¦èŒƒå›´ç”¨äºæŸå¤±å‡½æ•°
        height_range = max_height - min_height
        logger.info(f"ğŸ“Š é«˜åº¦èŒƒå›´è·¨åº¦: {height_range:.2f} ç±³")
        
        # âœ… è®¡ç®—å¹¶æ˜¾ç¤ºå½’ä¸€åŒ–é˜ˆå€¼ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        ground_norm_threshold = 5.0 / height_range if height_range > 0 else 0.05
        low_norm_threshold = 20.0 / height_range if height_range > 0 else 0.2
        mid_norm_threshold = 50.0 / height_range if height_range > 0 else 0.5
        high_norm_threshold = min(0.8, 80.0 / height_range) if height_range > 0 else 0.8
        
        logger.info(f"ğŸ¯ å½’ä¸€åŒ–é˜ˆå€¼æ˜ å°„:")
        logger.info(f"   åœ°é¢å±‚ (0-5m): å½’ä¸€åŒ–å€¼ â‰¤ {ground_norm_threshold:.3f}")
        logger.info(f"   ä½å»ºç­‘ (5-20m): å½’ä¸€åŒ–å€¼ {ground_norm_threshold:.3f} - {low_norm_threshold:.3f}")
        logger.info(f"   ä¸­å»ºç­‘ (20-50m): å½’ä¸€åŒ–å€¼ {low_norm_threshold:.3f} - {mid_norm_threshold:.3f}")
        logger.info(f"   é«˜å»ºç­‘ (50-80m): å½’ä¸€åŒ–å€¼ {mid_norm_threshold:.3f} - {high_norm_threshold:.3f}")
        logger.info(f"   è¶…é«˜å»ºç­‘ (>80m): å½’ä¸€åŒ–å€¼ > {high_norm_threshold:.3f}")        
        
        
        logger.info("âœ“ è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"åˆ›å»ºè®­ç»ƒæ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    val_loader = None
    if os.path.exists(val_image_dir) and os.path.exists(val_label_dir):
        logger.info("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        try:
            val_loader, _ = create_gamus_dataloader(
                image_dir=val_image_dir,
                label_dir=val_label_dir,
                batch_size=args.batch_size,
                shuffle=False,
                normalization_method=args.normalization_method,
                enable_augmentation=False,
                stats_json_path=args.stats_json_path,
                height_filter=height_filter,
                force_recompute=False,
                num_workers=min(args.num_workers, 2)
            )
            logger.info(f"âœ“ éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(val_loader.dataset)} ä¸ªæ ·æœ¬")
        except Exception as e:
            logger.error(f"åˆ›å»ºéªŒè¯æ•°æ®é›†å¤±è´¥: {e}")
            val_loader = None
    else:
        logger.warning("æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†è¿›è¡ŒéªŒè¯")
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    logger.info(f"æ•°æ®èŒƒå›´: [{height_filter['min_height']}, {height_filter['max_height']}] ç±³")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    logger.info("æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        test_batch = next(iter(train_loader))
        logger.info(f"âœ“ æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸ: å›¾åƒ {test_batch[0].shape}, æ ‡ç­¾ {test_batch[1].shape}")
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        raise
    
    return train_loader, val_loader, train_dataset, height_normalizer, min_height, max_height
class SimpleGAMUSValidator:
    """ç®€åŒ–çš„GAMUSéªŒè¯å™¨"""
    
    def __init__(self, height_normalizer, logger=None):
        self.height_normalizer = height_normalizer
        self.logger = logger or logging.getLogger(__name__)
        
    def denormalize_height(self, normalized_data):
        """ä½¿ç”¨å½’ä¸€åŒ–å™¨å°†å½’ä¸€åŒ–çš„nDSMæ•°æ®è¿˜åŸåˆ°çœŸå®é«˜åº¦å€¼"""
        return self.height_normalizer.denormalize(normalized_data)
    
    def validate_with_metrics(self, model, val_loader, criterion, device, epoch=None):
        """æ‰§è¡ŒéªŒè¯å¹¶è¿”å›è¯¦ç»†æŒ‡æ ‡"""
        model.eval()
        
        total_loss = 0.0
        total_count = 0
        
        # æ”¶é›†å°‘é‡æ ·æœ¬ç”¨äºæŒ‡æ ‡è®¡ç®—ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        all_preds_real = []
        all_targets_real = []
        max_samples = 100000  # é™åˆ¶æ ·æœ¬æ•°é‡
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f'Validation', leave=False)
            
            for batch_idx, (images, labels) in enumerate(pbar):
                images = images.to(device)
                labels = labels.to(device)
                
                try:
                    # å‰å‘ä¼ æ’­
                    predictions = model(images)
                    
                    # æ£€æŸ¥é¢„æµ‹å€¼
                    if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                        self.logger.warning(f"é¢„æµ‹å€¼åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                        continue
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(predictions, labels)
                    
                    # æ£€æŸ¥æŸå¤±å€¼
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.warning(f"æŸå¤±å€¼æ— æ•ˆï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                        continue
                    
                    total_loss += loss.item()
                    total_count += 1
                    
                    # æ”¶é›†æ ·æœ¬ç”¨äºæŒ‡æ ‡è®¡ç®—ï¼ˆé‡‡æ ·ä»¥èŠ‚çœå†…å­˜ï¼‰
                    if len(all_preds_real) < max_samples:
                        # è½¬æ¢ä¸ºnumpyå¹¶åå½’ä¸€åŒ–
                        preds_cpu = predictions.detach().cpu().numpy().flatten()
                        targets_cpu = labels.detach().cpu().numpy().flatten()
                        
                        # é‡‡æ ·
                        n_samples = min(500, len(preds_cpu))  # æ¯ä¸ªæ‰¹æ¬¡æœ€å¤š500ä¸ªæ ·æœ¬
                        if len(preds_cpu) > n_samples:
                            indices = np.random.choice(len(preds_cpu), n_samples, replace=False)
                            preds_cpu = preds_cpu[indices]
                            targets_cpu = targets_cpu[indices]
                        
                        # åå½’ä¸€åŒ–åˆ°çœŸå®é«˜åº¦
                        preds_real = self.denormalize_height(preds_cpu)
                        targets_real = self.denormalize_height(targets_cpu)
                        
                        all_preds_real.extend(preds_real)
                        all_targets_real.extend(targets_real)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({'loss': f'{loss.item():.6f}'})
                    
                except Exception as e:
                    self.logger.warning(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} é”™è¯¯: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / total_count if total_count > 0 else float('inf')
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        metrics = {'loss': avg_loss}
        
        if all_preds_real and all_targets_real:
            all_preds_real = np.array(all_preds_real)
            all_targets_real = np.array(all_targets_real)
            
            # ç§»é™¤æ— æ•ˆå€¼
            valid_mask = (~np.isnan(all_preds_real) & ~np.isnan(all_targets_real) & 
                         ~np.isinf(all_preds_real) & ~np.isinf(all_targets_real))
            
            if np.sum(valid_mask) > 10:
                valid_preds = all_preds_real[valid_mask]
                valid_targets = all_targets_real[valid_mask]
                
                # åŸºç¡€æŒ‡æ ‡
                mae = mean_absolute_error(valid_targets, valid_preds)
                mse = mean_squared_error(valid_targets, valid_preds)
                rmse = np.sqrt(mse)
                
                # RÂ²
                ss_res = np.sum((valid_targets - valid_preds) ** 2)
                ss_tot = np.sum((valid_targets - np.mean(valid_targets)) ** 2)
                r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-10 else 0.0
                
                # ç²¾åº¦æŒ‡æ ‡
                errors = np.abs(valid_preds - valid_targets)
                accuracy_1m = np.mean(errors <= 1.0)
                accuracy_2m = np.mean(errors <= 2.0)
                accuracy_5m = np.mean(errors <= 5.0)
                
                # åˆ†å±‚è¯¯å·®
                ground_mask = (valid_targets >= -5) & (valid_targets <= 5)
                low_mask = (valid_targets > 5) & (valid_targets <= 20)
                mid_mask = (valid_targets > 20) & (valid_targets <= 50)
                high_mask = valid_targets > 50
                
                mae_ground = np.mean(errors[ground_mask]) if np.sum(ground_mask) > 0 else 0.0
                mae_low = np.mean(errors[low_mask]) if np.sum(low_mask) > 0 else 0.0
                mae_mid = np.mean(errors[mid_mask]) if np.sum(mid_mask) > 0 else 0.0
                mae_high = np.mean(errors[high_mask]) if np.sum(high_mask) > 0 else 0.0
                
                metrics.update({
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'accuracy_1m': accuracy_1m,
                    'accuracy_2m': accuracy_2m,
                    'accuracy_5m': accuracy_5m,
                    'mae_ground': mae_ground,
                    'mae_low': mae_low,
                    'mae_mid': mae_mid,
                    'mae_high': mae_high,
                    'valid_samples': len(valid_preds),
                    'data_range': f'[{valid_targets.min():.1f}, {valid_targets.max():.1f}]m'
                })
        
        return metrics
    
    def log_metrics(self, epoch, metrics, is_best=False):
        """è®°å½•æŒ‡æ ‡"""
        self.logger.info(f'éªŒè¯æŒ‡æ ‡ - Epoch {epoch}:')
        self.logger.info(f'  æŸå¤±: {metrics.get("loss", "N/A"):.6f}')
        if 'mae' in metrics:
            self.logger.info(f'  MAE: {metrics["mae"]:.4f} m')
            self.logger.info(f'  RMSE: {metrics["rmse"]:.4f} m')
            self.logger.info(f'  RÂ²: {metrics["r2"]:.4f}')
            self.logger.info(f'  ç²¾åº¦: Â±1m={metrics["accuracy_1m"]:.1%}, Â±2m={metrics["accuracy_2m"]:.1%}, Â±5m={metrics["accuracy_5m"]:.1%}')
            self.logger.info(f'  åˆ†å±‚MAE: åœ°é¢={metrics["mae_ground"]:.2f}m, ä½å»ºç­‘={metrics["mae_low"]:.2f}m, ä¸­å»ºç­‘={metrics["mae_mid"]:.2f}m, é«˜å»ºç­‘={metrics["mae_high"]:.2f}m')
            self.logger.info(f'  æ•°æ®èŒƒå›´: {metrics["data_range"]}, æœ‰æ•ˆæ ·æœ¬: {metrics["valid_samples"]}')
        
        if is_best:
            self.logger.info('  â˜… æœ€ä½³éªŒè¯æ€§èƒ½ â˜…')
def validate_model_enhanced(model, val_loader, criterion, device, logger, height_normalizer, epoch=None):
    """å¢å¼ºçš„éªŒè¯å‡½æ•°"""
    if val_loader is None:
        return {'loss': 0.0, 'count': 0}
    
    # åˆ›å»ºç®€åŒ–çš„éªŒè¯å™¨
    validator = SimpleGAMUSValidator(height_normalizer, logger)
    
    # æ‰§è¡ŒéªŒè¯
    metrics = validator.validate_with_metrics(model, val_loader, criterion, device, epoch)
    
    # è®°å½•æŒ‡æ ‡
    validator.log_metrics(epoch, metrics)
    
    return metrics

def train_epoch(model, train_loader, criterion, optimizer, device, logger, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    total_count = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    
    for batch_idx, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.to(device)
        
        try:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            predictions = model(images)
            
            # æ£€æŸ¥é¢„æµ‹å€¼
            if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                logger.warning(f"é¢„æµ‹å€¼åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                continue
            
            # è®¡ç®—æŸå¤±
            loss = criterion(predictions, labels)
            
            # æ£€æŸ¥æŸå¤±å€¼
            if torch.isnan(loss) or torch.isinf(loss) or loss > 10:
                logger.warning(f"å¼‚å¸¸æŸå¤±å€¼ {loss.item():.6f}ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.6f}',
                'avg_loss': f'{total_loss/total_count:.6f}'
            })
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error(f"GPUå†…å­˜ä¸è¶³: {e}")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
            else:
                logger.error(f"è®­ç»ƒé”™è¯¯: {e}")
                continue
        except Exception as e:
            logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
            continue
    
    avg_loss = total_loss / total_count if total_count > 0 else float('inf')
    logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    return avg_loss

def save_checkpoint(epoch, model, optimizer, loss, save_dir, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
    latest_path = os.path.join(save_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_path = os.path.join(save_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)
        return best_path
    
    # å®šæœŸä¿å­˜
    if epoch % 10 == 0:
        epoch_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, epoch_path)
    
    return latest_path

def main():
    parser = argparse.ArgumentParser(description='ç®€åŒ–ç‰ˆGAMUS nDSMè®­ç»ƒè„šæœ¬')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--data_dir', type=str, default='/home/hudong26/HeightData/GAMUS/',
                        help='æ•°æ®æ ¹ç›®å½• (åŒ…å«imageså’Œheightå­ç›®å½•)')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--stats_json_path', type=str, default='./gamus_full_stats.json',
                        help='é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=40,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-5,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--num_workers', type=int, default=1,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--normalization_method', type=str, default='minmax',
                        choices=['minmax', 'percentile', 'zscore'],
                        help='å½’ä¸€åŒ–æ–¹æ³•')
    parser.add_argument('--min_height', type=float, default=-5.0,
                        help='æœ€å°é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--max_height', type=float, default=200.0,
                        help='æœ€å¤§é«˜åº¦è¿‡æ»¤å€¼ï¼ˆç±³ï¼‰')
    parser.add_argument('--enable_augmentation', action='store_true',
                        help='å¯ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--force_recompute_stats', action='store_true',
                        help='å¼ºåˆ¶é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--debug', action='store_true',
                        help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--encoder', type=str, default='vitb',
                        choices=['vits', 'vitb', 'vitl'],
                        help='ç¼–ç å™¨ç±»å‹')
    parser.add_argument('--pretrained_path', type=str, default='/home/hudong26/hudong26/Height/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth',
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--freeze_encoder', action='store_true',
                        help='å†»ç»“ç¼–ç å™¨')
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--loss_type', type=str, default='mse',
                        choices=['mse', 'mae', 'huber', 'focal', 'combined'],
                        help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--height_aware', action='store_true',
                        help='å¯ç”¨é«˜åº¦æ„ŸçŸ¥æŸå¤±')
    
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument('--patience', type=int, default=10,
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼')
    parser.add_argument('--early_stopping_patience', type=int, default=5,
                        help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--val_interval', type=int, default=1,
                        help='éªŒè¯é—´éš”')
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default='auto',
                        help='è®­ç»ƒè®¾å¤‡')
    
    args = parser.parse_args()
    # âœ… éªŒè¯å…³é”®å‚æ•°
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return 1
    
    # å¦‚æœæŒ‡å®šäº†ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶è·¯å¾„ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»™å‡ºæ˜ç¡®æç¤º
    if args.stats_json_path and not os.path.exists(args.stats_json_path):
        print(f"âŒ ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {args.stats_json_path}")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python precompute_stats.py {args.data_dir} --output {args.stats_json_path}")
        return 1    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(args.save_dir, f'training_{timestamp}.log')
    logger = setup_logger(log_file)
    
    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logging.getLogger('improved_dataset').setLevel(logging.DEBUG)
        logging.getLogger('improved_normalization_loss').setLevel(logging.DEBUG)
    
    # æ‰“å°é…ç½®
    logger.info("=" * 60)
    logger.info("ç®€åŒ–ç‰ˆGAMUS nDSMè®­ç»ƒ")
    logger.info("=" * 60)
    logger.info(f"é…ç½®å‚æ•°: {json.dumps(vars(args), indent=2, ensure_ascii=False)}")
    
    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        logger.info(f"GPUä¿¡æ¯: {torch.cuda.get_device_name()}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        # train_loader, val_loader, train_dataset = create_datasets(args, logger)
        train_loader, val_loader, train_dataset, height_normalizer, min_height, max_height = create_datasets(args, logger)
        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model = create_gamus_model(
            encoder=args.encoder,
            pretrained_path=args.pretrained_path,
            freeze_encoder=args.freeze_encoder
        ).to(device)
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"æ¨¡å‹å‚æ•°: æ€»æ•°={total_params:,}, å¯è®­ç»ƒ={trainable_params:,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=args.learning_rate,
            weight_decay=1e-4
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=args.patience,
            min_lr=1e-6,
            verbose=True
        )
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        criterion = create_height_loss(
            loss_type=args.loss_type,
            height_aware=args.height_aware,
            height_normalizer=height_normalizer,  # ä¼ å…¥å½’ä¸€åŒ–å™¨
            min_height=min_height,                # ä¼ å…¥çœŸå®æœ€å°é«˜åº¦
            max_height=max_height                 # ä¼ å…¥çœŸå®æœ€å¤§é«˜åº¦
        )
        logger.info(f"ğŸ“Š æŸå¤±å‡½æ•°é…ç½®:")
        logger.info(f"   ç±»å‹: {args.loss_type}")
        logger.info(f"   é«˜åº¦æ„ŸçŸ¥: {args.height_aware}")
        logger.info(f"   é«˜åº¦èŒƒå›´: [{min_height:.2f}, {max_height:.2f}] ç±³")
        # è®­ç»ƒå¾ªç¯
        logger.info("å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        start_time = time.time()
        
        for epoch in range(1, args.num_epochs + 1):
            # è®­ç»ƒ
            train_loss = train_epoch(
                model, train_loader, criterion, optimizer, device, logger, epoch
            )
            
            # éªŒè¯
            if epoch % args.val_interval == 0:
                val_metrics = validate_model_enhanced(
                    model, val_loader, criterion, device, logger, height_normalizer, epoch
                )
                val_loss = val_metrics['loss']
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_loss)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                saved_path = save_checkpoint(
                    epoch, model, optimizer, val_loss, args.save_dir, is_best
                )
                
                # æ‰“å°ç»“æœ
                logger.info(f"Epoch {epoch}/{args.num_epochs} ç»“æœ:")
                logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
                logger.info(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
                logger.info(f"  {'ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹!' if is_best else ''}")
                logger.info(f"  ä¿å­˜è·¯å¾„: {saved_path}")
                logger.info("-" * 60)
                
                # æ—©åœæ£€æŸ¥
                if patience_counter >= args.early_stopping_patience:
                    logger.info(f"æ—©åœè§¦å‘ (patience: {patience_counter})")
                    break
            # æ¸…ç†GPUå†…å­˜
            if device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        logger.info(f"è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ€»è€—æ—¶: {total_time / 3600:.2f} å°æ—¶")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"æ¨¡å‹ä¿å­˜åœ¨: {args.save_dir}")
        
    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        logger.info("è®­ç»ƒè„šæœ¬å·²é€€å‡º")

if __name__ == '__main__':
    main()