import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from functools import partial
from pathlib import Path
from typing import Dict, Any, Optional, Union, Tuple, List

from .base_model import BaseDepthModel
from .multi_scale_loss import get_loss_function, SingleScaleLoss
from .scale_modulator import ScaleModulator
from .decoder import ResolutionAgnosticDecoder

class Depth2ElevationEncoder(nn.Module):
    """åŸºäºŽDINOv2çš„é«˜ç¨‹ç¼–ç å™¨ï¼Œé›†æˆScale Modulator"""
    def __init__(self, 
                 embed_dim: int,
                 num_heads: int,
                 img_size: int = 448,
                 patch_size: int = 14,
                 in_chans: int = 3,
                 num_register_tokens: int = 0,
                 interpolate_antialias: bool = False,
                 interpolate_offset: float = 0.1):
        super().__init__()
        
        # Patch embedding (åŸºäºŽDINOv2)
        self.patch_embed = PatchEmbed(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim
        )
        
        num_patches = self.patch_embed.num_patches
        self.num_tokens = 1
        self.num_register_tokens = num_register_tokens
        self.interpolate_antialias = interpolate_antialias
        self.interpolate_offset = interpolate_offset
        
        # Position embeddings
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        
        if num_register_tokens > 0:
            self.register_tokens = nn.Parameter(torch.zeros(1, num_register_tokens, embed_dim))
        else:
            self.register_tokens = None
        
        # ç”¨Scale Modulatoræ›¿æ¢åŽŸæ¥çš„blocks
        self.scale_modulator = ScaleModulator(embed_dim, num_heads)
        
        # è¾“å‡ºnorm
        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)
        
        # åˆå§‹åŒ–
        self.init_weights()
        
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        torch.nn.init.trunc_normal_(self.pos_embed, std=0.02)
        torch.nn.init.normal_(self.cls_token, std=1e-6)
        if self.register_tokens is not None:
            torch.nn.init.normal_(self.register_tokens, std=1e-6)
        
    def interpolate_pos_encoding(self, x: torch.Tensor, w: int, h: int) -> torch.Tensor:
        """ä½ç½®ç¼–ç æ’å€¼ï¼Œä¿æŒåŽŸDINOv2çš„é€»è¾‘"""
        previous_dtype = x.dtype
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
            
        pos_embed = self.pos_embed.float()
        class_pos_embed = pos_embed[:, 0]
        patch_pos_embed = pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // self.patch_embed.patch_size[0]
        h0 = h // self.patch_embed.patch_size[1]
        w0, h0 = w0 + self.interpolate_offset, h0 + self.interpolate_offset
        
        sqrt_N = math.sqrt(N)
        sx, sy = float(w0) / sqrt_N, float(h0) / sqrt_N
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(sqrt_N), int(sqrt_N), dim).permute(0, 3, 1, 2),
            scale_factor=(sx, sy),
            mode="bicubic",
            antialias=self.interpolate_antialias
        )
        
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1).to(previous_dtype)
        
    def prepare_tokens_with_masks(self, x: torch.Tensor, masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‡†å¤‡è¾“å…¥tokensï¼Œä¿æŒåŽŸDINOv2çš„é€»è¾‘"""
        B, nc, w, h = x.shape
        x = self.patch_embed(x)
        
        if masks is not None:
            # å¦‚æžœæœ‰mask tokenï¼Œåº”ç”¨æŽ©ç 
            mask_token = getattr(self, 'mask_token', None)
            if mask_token is not None:
                x = torch.where(masks.unsqueeze(-1), mask_token.to(x.dtype).unsqueeze(0), x)

        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
        x = x + self.interpolate_pos_encoding(x, w, h)

        if self.register_tokens is not None:
            x = torch.cat((
                x[:, :1],
                self.register_tokens.expand(x.shape[0], -1, -1),
                x[:, 1:],
            ), dim=1)

        return x
        
    def forward(self, x: torch.Tensor, masks: Optional[torch.Tensor] = None) -> List[torch.Tensor]:
        """
        Args:
            x: è¾“å…¥å›¾åƒ [B, 3, H, W] 
            masks: å¯é€‰æŽ©ç 
        Returns:
            list: 4ä¸ªå°ºåº¦çš„ç‰¹å¾ [fs1, fs2, fs3, fs4]
        """
        x = self.prepare_tokens_with_masks(x, masks)
        
        # é€šè¿‡Scale ModulatorèŽ·å–å¤šå°ºåº¦ç‰¹å¾
        scale_features = self.scale_modulator(x)
        
        # å¯¹æ¯ä¸ªå°ºåº¦ç‰¹å¾è¿›è¡Œnorm
        scale_features = [self.norm(feat) for feat in scale_features]
        
        return scale_features

class PatchEmbed(nn.Module):
    """2D Image to Patch Embedding"""
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = (img_size, img_size) if isinstance(img_size, int) else img_size
        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size
        self.grid_size = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)  # BCHW -> BNC
        return x

class Depth2Elevation(BaseDepthModel):
    """Depth2Elevationä¸»æ¨¡åž‹ - åŸºäºŽè®ºæ–‡å®Œæ•´å®žçŽ°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        model_config = config.get('model_config', {})
        
        # æ¨¡åž‹å‚æ•°
        self.encoder_type = model_config.get('encoder', 'vitb')
        self.img_size = model_config.get('img_size', 448)
        self.patch_size = model_config.get('patch_size', 14)
        self.use_multi_scale_output = config.get('use_multi_scale_output', True)
        
        # ç¡®å®šæ¨¡åž‹é…ç½®
        model_configs = {
            'vits': {'embed_dim': 384, 'num_heads': 6},
            'vitb': {'embed_dim': 768, 'num_heads': 12},
            'vitl': {'embed_dim': 1024, 'num_heads': 16},
            'vitg': {'embed_dim': 1536, 'num_heads': 24}
        }
        
        if self.encoder_type not in model_configs:
            raise ValueError(f"Unknown encoder type: {self.encoder_type}")
        
        encoder_config = model_configs[self.encoder_type]
        self.embed_dim = encoder_config['embed_dim']
        self.num_heads = encoder_config['num_heads']
        
        # åˆ›å»ºç¼–ç å™¨
        self.height_encoder = Depth2ElevationEncoder(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            img_size=self.img_size,
            patch_size=self.patch_size,
            num_register_tokens=0  # å¯é…ç½®
        )
        
        # åˆ›å»ºè§£ç å™¨
        self.decoder = ResolutionAgnosticDecoder(
            embed_dim=self.embed_dim,
            num_register_tokens=0
        )
        
        # æŸå¤±å‡½æ•°
        loss_config = config.get('loss_config', {})
        self.loss_fn = get_loss_function(loss_config)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        pretrained_path = model_config.get('pretrained_path')
        if pretrained_path and Path(pretrained_path).exists():
            self.load_pretrained_weights(pretrained_path)
        
        # # æ˜¯å¦å†»ç»“ç¼–ç å™¨
        # if model_config.get('freeze_encoder', False):
        #     self.freeze_encoder()
        # èŽ·å–å†»ç»“ç­–ç•¥é…ç½®
        freezing_config = config.get('freezing_config', {})
        freezing_strategy = freezing_config.get('strategy', 'none')
        
        # å‘åŽå…¼å®¹ï¼šå¦‚æžœmodel_configä¸­è®¾ç½®äº†freeze_encoder=Trueï¼Œä¸”æ²¡æœ‰æ˜Žç¡®è®¾ç½®strategy
        if (model_config.get('freeze_encoder', False) and 
            freezing_strategy == 'none' and 
            'strategy' not in freezing_config):
            freezing_strategy = 'simple'
            print("ðŸ“Œ Detected legacy freeze_encoder=True, using 'simple' strategy")
        
        # åº”ç”¨å†»ç»“ç­–ç•¥
        if freezing_strategy != 'none':
            self.apply_freezing_strategy(freezing_strategy)
    
    def load_pretrained_weights(self, pretrained_path: str):
        """åŠ è½½DAMé¢„è®­ç»ƒæƒé‡"""
        print(f"Loading pretrained weights from {pretrained_path}")
        
        try:
            checkpoint = torch.load(pretrained_path, map_location='cpu')
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # åŠ è½½ç¼–ç å™¨æƒé‡ï¼ˆå¿½ç•¥ä¸åŒ¹é…çš„é”®ï¼‰
            encoder_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('patch_embed') or key.startswith('cls_token') or key.startswith('pos_embed'):
                    encoder_state_dict[key] = value
                elif key.startswith('blocks'):
                    # å°†åŽŸæ¥çš„blocksæƒé‡æ˜ å°„åˆ°height blocks
                    new_key = key.replace('blocks', 'scale_modulator.height_blocks')
                    encoder_state_dict[new_key] = value
            
            # è½½å…¥æƒé‡ï¼Œå…è®¸éƒ¨åˆ†åŒ¹é…
            missing_keys, unexpected_keys = self.height_encoder.load_state_dict(encoder_state_dict, strict=False)
            
            print(f"Loaded pretrained weights. Missing keys: {len(missing_keys)}, Unexpected keys: {len(unexpected_keys)}")
            
        except Exception as e:
            print(f"Warning: Failed to load pretrained weights: {e}")
    
    def freeze_encoder(self):
        """å†»ç»“ç¼–ç å™¨å‚æ•°"""
        for param in self.height_encoder.parameters():
            param.requires_grad = False
        print("Encoder frozen")
    
    def unfreeze_encoder(self):
        """è§£å†»ç¼–ç å™¨å‚æ•°"""
        for param in self.height_encoder.parameters():
            param.requires_grad = True
        print("Encoder unfrozen")
    def freeze_encoder_selectively(self):
        """ç²¾ç»†åŒ–å†»ç»“ç­–ç•¥ - æŒ‰è®ºæ–‡è¦æ±‚å†»ç»“DAMåŽŸæœ‰ç»„ä»¶ï¼Œè§£å†»æ–°å¢žç»„ä»¶"""
        print("Applying selective encoder freezing strategy...")
        
        # 1. å†»ç»“åŸºç¡€ç»„ä»¶ï¼ˆä¿ç•™DAMçš„é¢„è®­ç»ƒçŸ¥è¯†ï¼‰
        for param in self.height_encoder.patch_embed.parameters():
            param.requires_grad = False
        print("  âœ“ Frozen patch_embed")
        
        # pos_embedå’Œcls_tokenå¯ä»¥å¾®è°ƒï¼ˆé¥æ„Ÿå›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„ç©ºé—´å…³ç³»ï¼‰
        self.height_encoder.pos_embed.requires_grad = True
        self.height_encoder.cls_token.requires_grad = True
        if hasattr(self.height_encoder, 'register_tokens') and self.height_encoder.register_tokens is not None:
            self.height_encoder.register_tokens.requires_grad = True
        print("  âœ“ Unfrozen pos_embed, cls_token")
        
        # 2. Height Blocksçš„ç²¾ç»†åŒ–å¤„ç†
        frozen_components = []
        trainable_components = []
        
        for i, height_block in enumerate(self.height_encoder.scale_modulator.height_blocks):
            # å†»ç»“DAMåŽŸæœ‰ç»„ä»¶
            for param in height_block.attn.parameters():
                param.requires_grad = False
            for param in height_block.norm1.parameters():
                param.requires_grad = False
            for param in height_block.norm2.parameters():
                param.requires_grad = False
            for param in height_block.mlp.parameters():
                param.requires_grad = False
            
            frozen_components.extend(['attn', 'norm1', 'norm2', 'mlp'])
            
            # è§£å†»æ–°å¢žçš„additional_mlpï¼ˆè®ºæ–‡æ˜Žç¡®è¯´æ˜Žæ˜¯trainableï¼‰
            for param in height_block.additional_mlp.parameters():
                param.requires_grad = True
            
            trainable_components.append(f'height_block_{i}.additional_mlp')
        
        print(f"  âœ“ Frozen DAM components in {len(self.height_encoder.scale_modulator.height_blocks)} height blocks")
        print(f"  âœ“ Unfrozen additional_mlp in {len(self.height_encoder.scale_modulator.height_blocks)} height blocks")
        
        # 3. è§£å†»æ‰€æœ‰scale_adaptersï¼ˆè®ºæ–‡è¯´æ˜Žéœ€è¦learnï¼‰
        for i, scale_adapter in enumerate(self.height_encoder.scale_modulator.scale_adapters):
            for param in scale_adapter.parameters():
                param.requires_grad = True
            trainable_components.append(f'scale_adapter_{i}')
        
        print(f"  âœ“ Unfrozen {len(self.height_encoder.scale_modulator.scale_adapters)} scale adapters")
        
        # 4. å†»ç»“æœ€åŽçš„normå±‚
        for param in self.height_encoder.norm.parameters():
            param.requires_grad = False
        print("  âœ“ Frozen final norm layer")
        
        # 5. ç¡®ä¿decoderå®Œå…¨å¯è®­ç»ƒ
        for param in self.decoder.parameters():
            param.requires_grad = True
        print("  âœ“ Decoder remains fully trainable")
        
        # 6. æ‰“å°å‚æ•°ç»Ÿè®¡
        self.print_trainable_parameters()
    def print_trainable_parameters(self):
        """æ‰“å°å¯è®­ç»ƒå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºŽéªŒè¯å†»ç»“ç­–ç•¥"""
        total_params = 0
        trainable_params = 0
        frozen_params = 0
        
        component_stats = {}
        
        for name, param in self.named_parameters():
            total_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
            else:
                frozen_params += param.numel()
            
            # ç»Ÿè®¡å„ç»„ä»¶çš„å‚æ•°æƒ…å†µ
            component = name.split('.')[0] if '.' in name else name
            if component not in component_stats:
                component_stats[component] = {'total': 0, 'trainable': 0}
            component_stats[component]['total'] += param.numel()
            if param.requires_grad:
                component_stats[component]['trainable'] += param.numel()
        
        print("\n" + "="*60)
        print("ðŸ“Š PARAMETER STATISTICS")
        print("="*60)
        print(f"Total Parameters:     {total_params:,}")
        print(f"Trainable Parameters: {trainable_params:,}")
        print(f"Frozen Parameters:    {frozen_params:,}")
        print(f"Trainable Ratio:      {trainable_params/total_params*100:.2f}%")
        
        print("\nðŸ“‹ Component Breakdown:")
        print("-"*40)
        for component, stats in component_stats.items():
            ratio = stats['trainable']/stats['total']*100
            status = "ðŸŸ¢" if ratio > 90 else "ðŸŸ¡" if ratio > 10 else "ðŸ”´"
            print(f"{status} {component:<20}: {stats['trainable']:>8,}/{stats['total']:>8,} ({ratio:>5.1f}%)")
        print("="*60 + "\n")

    def apply_freezing_strategy(self, strategy: str = "none"):
        """æ ¹æ®é…ç½®åº”ç”¨ä¸åŒçš„å†»ç»“ç­–ç•¥"""
        strategy = strategy.lower()
        
        if strategy == "none":
            print("ðŸ”“ No freezing applied - all parameters trainable")
            return
        elif strategy == "simple":
            self.freeze_encoder()
        elif strategy == "selective":
            self.freeze_encoder_selectively()
        else:
            raise ValueError(f"Unknown freezing strategy: {strategy}. "
                            f"Available: ['none', 'simple', 'selective']")
    def forward(self, 
               x: torch.Tensor, 
               return_multi_scale: bool = None,
               masks: Optional[torch.Tensor] = None) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            x: è¾“å…¥å›¾åƒ [B, 3, H, W]
            return_multi_scale: æ˜¯å¦è¿”å›žå¤šå°ºåº¦ç»“æžœ
            masks: å¯é€‰æŽ©ç 
        Returns:
            å¦‚æžœreturn_multi_scale=True: dict with multiple scales
            å¦åˆ™: æœ€é«˜åˆ†è¾¨çŽ‡çš„é«˜ç¨‹å›¾
        """
        if return_multi_scale is None:
            return_multi_scale = self.use_multi_scale_output
        
        # è®¡ç®—patchæ•°é‡
        patch_h, patch_w = x.shape[-2] // self.patch_size, x.shape[-1] // self.patch_size
        
        # Height Encoder: èŽ·å–å¤šå°ºåº¦ç‰¹å¾
        scale_features = self.height_encoder(x, masks)
        
        # Decoder: ç”Ÿæˆå¤šå°ºåº¦é«˜ç¨‹é¢„æµ‹
        predictions = self.decoder(scale_features, patch_h, patch_w)
        
        if return_multi_scale:
            return predictions
        else:
            # è¿”å›žæœ€é«˜åˆ†è¾¨çŽ‡çš„ç»“æžœ
            return predictions.get('scale_4', list(predictions.values())[-1])
    
    def compute_loss(self, 
                    predictions: Union[torch.Tensor, Dict[str, torch.Tensor]], 
                    targets: torch.Tensor,
                    masks: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
            # æ£€æŸ¥æŸå¤±å‡½æ•°ç±»åž‹
        if isinstance(self.loss_fn, SingleScaleLoss):
            # å•å°ºåº¦æŸå¤±ï¼šéœ€è¦æå–ä¸€ä¸ªtensor
            if isinstance(predictions, dict):
                # ä½¿ç”¨æœ€é«˜åˆ†è¾¨çŽ‡çš„é¢„æµ‹ï¼ˆé€šå¸¸æ˜¯scale_4æˆ–æœ€åŽä¸€ä¸ªï¼‰
                main_prediction = predictions.get('scale_4', list(predictions.values())[-1])
            else:
                main_prediction = predictions
            
            return self.loss_fn(main_prediction, targets, masks)
        
        else:
            # å¤šå°ºåº¦æŸå¤±ï¼šéœ€è¦å­—å…¸æ ¼å¼
            if isinstance(predictions, torch.Tensor):
                # å•å°ºåº¦é¢„æµ‹ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                predictions = {'scale_1': predictions}
            
            return self.loss_fn(predictions, targets, masks)

    
    def get_model_info(self) -> Dict[str, Any]:
        """èŽ·å–æ¨¡åž‹ä¿¡æ¯"""
        info = super().get_model_info()
        info.update({
            'encoder_type': self.encoder_type,
            'embed_dim': self.embed_dim,
            'num_heads': self.num_heads,
            'img_size': self.img_size,
            'patch_size': self.patch_size,
        })
        return info


def create_depth2elevation_model(config: Dict[str, Any]) -> Depth2Elevation:
    """åˆ›å»ºDepth2Elevationæ¨¡åž‹çš„å·¥åŽ‚å‡½æ•°"""
    return Depth2Elevation(config)