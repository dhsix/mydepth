import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset
from torchvision import transforms
import cv2
import logging
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
import warnings
from improved_normalization_loss import HeightNormalizer

warnings.filterwarnings('ignore')

class GAMUSDataset(Dataset):
    """ç®€åŒ–ç‰ˆGAMUS nDSMæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, 
                 image_dir: str, 
                 label_dir: str, 
                 normalization_method: str = 'percentile',
                 enable_augmentation: bool = False,
                 stats_json_path: Optional[str] = None,
                 height_filter: Optional[Dict[str, float]] = None,
                 force_recompute: bool = False):
        """
        åˆå§‹åŒ–GAMUSæ•°æ®é›†
        
        å‚æ•°:
            image_dir: å½±åƒåˆ‡ç‰‡ç›®å½•
            label_dir: nDSMæ ‡ç­¾åˆ‡ç‰‡ç›®å½•  
            normalization_method: å½’ä¸€åŒ–æ–¹æ³• ('percentile', 'minmax', 'zscore')
            enable_augmentation: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
            stats_json_path: ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„
            height_filter: é«˜åº¦è¿‡æ»¤é…ç½® {'min_height': -5.0, 'max_height': 100.0}
            force_recompute: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        """
        # âœ… æ·»åŠ loggeråˆå§‹åŒ–
        self.logger = logging.getLogger(self.__class__.__name__)
        # åœ¨__init__æœ€å¼€å§‹æ·»åŠ 
        if not stats_json_path:
            raise ValueError("å¿…é¡»æŒ‡å®šstats_json_pathå‚æ•°ï¼Œè¯·å…ˆè¿è¡Œé¢„è®¡ç®—è„šæœ¬")

        if not os.path.exists(stats_json_path):
            raise FileNotFoundError(f"ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {stats_json_path}ï¼Œè¯·å…ˆè¿è¡Œ: python precompute_stats.py {image_dir}/../..")
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.normalization_method = normalization_method
        self.enable_augmentation = enable_augmentation
        self.stats_json_path = stats_json_path
        self.force_recompute = force_recompute

        # è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨ï¼ˆé»˜è®¤åˆç†èŒƒå›´ï¼‰
        self.height_filter = height_filter or {'min_height': -5.0, 'max_height': 100.0}
        
        # éªŒè¯ç›®å½•
        self._validate_directories()
        
        # è·å–æ–‡ä»¶å¯¹
        self.file_pairs = self._get_file_pairs()
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯å’Œå½’ä¸€åŒ–å™¨
        self._initialize_normalizer()
        
        # è®¾ç½®æ•°æ®å˜æ¢
        self.transform = self._setup_transforms()
        self.logger.info(f"GAMUSæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self.file_pairs)} ä¸ªæ ·æœ¬")
     
    def _validate_directories(self):
        """éªŒè¯ç›®å½•å­˜åœ¨æ€§"""
        if not os.path.exists(self.image_dir):
            raise FileNotFoundError(f"å½±åƒç›®å½•ä¸å­˜åœ¨: {self.image_dir}")
        if not os.path.exists(self.label_dir):
            raise FileNotFoundError(f"nDSMæ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {self.label_dir}")
    
    def _get_file_pairs(self):
        """è·å–åŒ¹é…çš„æ–‡ä»¶å¯¹"""
        # è·å–æ–‡ä»¶åˆ—è¡¨
        image_files = [f for f in os.listdir(self.image_dir) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))]
        label_files = [f for f in os.listdir(self.label_dir) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))]
        
        # åˆ›å»ºåŒ¹é…æ˜ å°„
        image_dict = {}
        label_dict = {}
        
        # å¤„ç†å›¾åƒæ–‡ä»¶
        for img_file in image_files:
            # å¤„ç† DC_02_26_RGB_r0c0.jpg -> DC_02_26_r0c0
            if '_RGB_' in img_file:
                base_name = img_file.replace('_RGB_', '_').rsplit('.', 1)[0]
            else:
                base_name = Path(img_file).stem
            image_dict[base_name] = img_file
        
        # å¤„ç†æ ‡ç­¾æ–‡ä»¶  
        for label_file in label_files:
            # å¤„ç† DC_02_26_AGL_r0c0.png -> DC_02_26_r0c0
            if '_AGL_' in label_file:
                base_name = label_file.replace('_AGL_', '_').rsplit('.', 1)[0]
            else:
                base_name = Path(label_file).stem
            label_dict[base_name] = label_file
        
        # æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¯¹
        matched_pairs = []
        for base_name in image_dict:
            if base_name in label_dict:
                matched_pairs.append((image_dict[base_name], label_dict[base_name]))
        
        if not matched_pairs:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å›¾åƒ-nDSMæ ‡ç­¾å¯¹")
        
        logging.info(f"æˆåŠŸåŒ¹é… {len(matched_pairs)} ä¸ªæ–‡ä»¶å¯¹")
        return matched_pairs
    
    def _initialize_normalizer(self):
        """åˆå§‹åŒ–å½’ä¸€åŒ–å™¨"""

        # å°è¯•ä»JSONåŠ è½½ç»Ÿè®¡ä¿¡æ¯
        if self.stats_json_path and os.path.exists(self.stats_json_path):
            self.logger.info(f"ğŸ”„ åŠ è½½é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯: {self.stats_json_path}")
            if self._load_from_json():
                self.logger.info("âœ… æˆåŠŸåŠ è½½é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œè·³è¿‡é‡æ–°è®¡ç®—")
                return
            else:
                self.logger.warning("âš ï¸ é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯åŠ è½½å¤±è´¥")
        
        # å¦‚æœå¼ºåˆ¶é‡æ–°è®¡ç®—æˆ–æ²¡æœ‰é¢„è®¡ç®—æ–‡ä»¶
        if self.force_recompute or not self.stats_json_path or not os.path.exists(self.stats_json_path):
            self.logger.error("âŒ æ²¡æœ‰é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ä¸”ç¦ç”¨å®æ—¶è®¡ç®—")
            self.logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python precompute_stats.py <data_dir>")
            raise ValueError("éœ€è¦å…ˆè¿è¡Œé¢„è®¡ç®—è„šæœ¬ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
    
    def _load_from_json(self) -> bool:
        """ä»JSONæ–‡ä»¶åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with open(self.stats_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯JSONæ ¼å¼
            if 'global_statistics' not in data:
                logging.warning("JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                return False
            
            stats = data['global_statistics']
            
            # åŠ è½½åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            self.global_min = float(stats['min'])
            self.global_max = float(stats['max'])
            self.global_mean = float(stats['mean'])
            self.global_std = float(stats['std'])
            
            # åº”ç”¨é«˜åº¦è¿‡æ»¤
            # original_min, original_max = self.global_min, self.global_max
            precomputed_filter = data.get('processing_info', {}).get('height_filter', {})
            if (precomputed_filter.get('min_height') != self.height_filter['min_height'] or 
                precomputed_filter.get('max_height') != self.height_filter['max_height']):
                self.logger.warning(f"âš ï¸ é«˜åº¦è¿‡æ»¤è®¾ç½®ä¸é¢„è®¡ç®—ä¸ä¸€è‡´")
                self.logger.warning(f"   é¢„è®¡ç®—: {precomputed_filter}")
                self.logger.warning(f"   å½“å‰è®¾ç½®: {self.height_filter}")

            original_min, original_max = self.global_min, self.global_max
            self.global_min = max(self.global_min, self.height_filter['min_height'])
            self.global_max = min(self.global_max, self.height_filter['max_height'])
            
            if self.global_min != original_min or self.global_max != original_max:
                self.logger.info(f"åº”ç”¨é«˜åº¦è¿‡æ»¤: [{original_min:.1f}, {original_max:.1f}] -> [{self.global_min:.1f}, {self.global_max:.1f}]")
            
            # åˆ›å»ºå½’ä¸€åŒ–å™¨
            self.height_normalizer = HeightNormalizer(self.normalization_method)
            
            # ä½¿ç”¨è¿‡æ»¤åçš„èŒƒå›´é‡æ–°æ‹Ÿåˆå½’ä¸€åŒ–å™¨
            dummy_data = np.linspace(self.global_min, self.global_max, 1000)
            self.height_normalizer.fit(dummy_data)
            
            # logging.info(f"æ•°æ®èŒƒå›´: [{self.global_min:.1f}, {self.global_max:.1f}] ç±³")
            # è¾“å‡ºåŠ è½½çš„ç»Ÿè®¡ä¿¡æ¯
            processing_info = data.get('processing_info', {})
            self.logger.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ä¿¡æ¯:")
            self.logger.info(f"   æ•°æ®èŒƒå›´: [{self.global_min:.2f}, {self.global_max:.2f}] ç±³")
            self.logger.info(f"   å¤„ç†æ–‡ä»¶: {processing_info.get('processed_files', '?')} ä¸ª")
            self.logger.info(f"   æ•°æ®ç‚¹æ•°: {stats.get('total_samples', '?'):,}")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½JSONç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def _compute_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        all_heights = []
        
        logging.info(f"å¤„ç† {len(self.file_pairs)} ä¸ªæ–‡ä»¶...")
        
        for idx, (_, label_file) in enumerate(self.file_pairs):
            if idx % 500 == 0:
                logging.info(f"è¿›åº¦: {idx+1}/{len(self.file_pairs)}")
            
            try:
                label_path = os.path.join(self.label_dir, label_file)
                label_data = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)
                
                if label_data is None:
                    continue
                
                # è½¬æ¢æ•°æ®ç±»å‹
                label_data = label_data.astype(np.float32)
                
                # è¿‡æ»¤æ— æ•ˆå€¼
                valid_mask = self._get_valid_mask(label_data)
                valid_heights = label_data[valid_mask]
                
                if len(valid_heights) > 0:
                    # å•ä½è½¬æ¢ï¼šå˜ç±³è½¬ç±³
                    valid_heights = valid_heights / 100.0
                    
                    # åº”ç”¨é«˜åº¦è¿‡æ»¤
                    filtered_heights = valid_heights[
                        (valid_heights >= self.height_filter['min_height']) & 
                        (valid_heights <= self.height_filter['max_height'])
                    ]
                    
                    if len(filtered_heights) > 0:
                        all_heights.append(filtered_heights)
                        
            except Exception as e:
                logging.warning(f"å¤„ç†æ–‡ä»¶ {label_file} å¤±è´¥: {e}")
                continue
        
        if not all_heights:
            raise ValueError("æ— æ³•æå–æœ‰æ•ˆçš„é«˜åº¦æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_heights = np.concatenate(all_heights)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.global_min = float(np.min(combined_heights))
        self.global_max = float(np.max(combined_heights))
        self.global_mean = float(np.mean(combined_heights))
        self.global_std = float(np.std(combined_heights))
        
        # åˆ›å»ºå¹¶è®­ç»ƒå½’ä¸€åŒ–å™¨
        self.height_normalizer = HeightNormalizer(self.normalization_method)
        self.height_normalizer.fit(combined_heights)
        
        logging.info(f"ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ:")
        logging.info(f"  æ•°æ®èŒƒå›´: [{self.global_min:.1f}, {self.global_max:.1f}] ç±³")
        logging.info(f"  å‡å€¼Â±æ ‡å‡†å·®: {self.global_mean:.1f} Â± {self.global_std:.1f} ç±³")
        logging.info(f"  æœ‰æ•ˆæ•°æ®ç‚¹: {len(combined_heights):,} ä¸ª")
    
    def _get_valid_mask(self, data):
        """è·å–æœ‰æ•ˆæ•°æ®æ©ç """
        invalid_values = [-9999, -32768, 9999, 32767]
        valid_mask = ~(np.isnan(data) | np.isinf(data))
        
        for invalid_val in invalid_values:
            valid_mask = valid_mask & (data != invalid_val)
        
        return valid_mask
    
    def _save_to_json(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        try:
            data = {
                "timestamp": self._get_timestamp(),
                "processing_info": {
                    "total_files": len(self.file_pairs),
                    "height_filter": self.height_filter,
                    "normalization_method": self.normalization_method
                },
                "global_statistics": {
                    "min": float(self.global_min),
                    "max": float(self.global_max),
                    "mean": float(self.global_mean),
                    "std": float(self.global_std)
                }
            }
            
            os.makedirs(os.path.dirname(self.stats_json_path), exist_ok=True)
            
            with open(self.stats_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logging.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {self.stats_json_path}")
            
        except Exception as e:
            logging.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
    
    def _get_timestamp(self):
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def _setup_transforms(self):
        """è®¾ç½®æ•°æ®å˜æ¢"""
        transform_list = []
        
        if self.enable_augmentation:
            transform_list.extend([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.5),
                transforms.ColorJitter(brightness=0.03, contrast=0.03, saturation=0.03, hue=0.01),
                transforms.RandomRotation(degrees=5, fill=0),
            ])
        
        transform_list.append(
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        )
        
        return transforms.Compose(transform_list)
    
    def __len__(self):
        return len(self.file_pairs)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        try:
            image_file, label_file = self.file_pairs[idx]
            
            # åŠ è½½å½±åƒ
            image = self._load_image(os.path.join(self.image_dir, image_file))
            
            # åŠ è½½nDSMæ ‡ç­¾
            label = self._load_label(os.path.join(self.label_dir, label_file))
            
            # è½¬æ¢ä¸ºtensor
            image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()
            label_tensor = torch.from_numpy(label).float()
            
            # åº”ç”¨å˜æ¢
            if self.transform:
                image_tensor = self.transform(image_tensor)
            
            return image_tensor, label_tensor
            
        except Exception as e:
            logging.error(f"åŠ è½½æ ·æœ¬ {idx} å¤±è´¥: {e}")
            # è¿”å›ä¸‹ä¸€ä¸ªæ ·æœ¬
            return self.__getitem__((idx + 1) % len(self.file_pairs))
    
    def _load_image(self, image_path):
        """åŠ è½½å½±åƒæ–‡ä»¶"""
        image = cv2.imread(image_path, cv2.IMREAD_COLOR)
        if image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å½±åƒ: {image_path}")
        
        # é¢œè‰²ç©ºé—´è½¬æ¢å’Œå½’ä¸€åŒ–
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
        
        # è°ƒæ•´å¤§å°
        if image.shape[:2] != (448, 448):
            image = cv2.resize(image, (448, 448), interpolation=cv2.INTER_CUBIC)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        if image.max() > 1:
            image = image / 255.0
        
        return image
    
    def _load_label(self, label_path):
        """åŠ è½½nDSMæ ‡ç­¾æ–‡ä»¶"""
        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)
        if label is None:
            raise ValueError(f"æ— æ³•åŠ è½½nDSMæ ‡ç­¾: {label_path}")
        
        label = label.astype(np.float32)
        
        # è°ƒæ•´å¤§å°
        if label.shape != (448, 448):
            label = cv2.resize(label, (448, 448), interpolation=cv2.INTER_LINEAR)
        
        # å¤„ç†æ— æ•ˆå€¼
        INVALID_MARKER = -999.0
        valid_mask = self._get_valid_mask(label)
        
        # æ ‡è®°æ— æ•ˆå€¼
        label[~valid_mask] = INVALID_MARKER
        
        # å•ä½è½¬æ¢ï¼šå˜ç±³è½¬ç±³
        label[valid_mask] = label[valid_mask] / 100.0
        
        # åº”ç”¨é«˜åº¦è¿‡æ»¤
        height_valid_mask = (
            (label >= self.height_filter['min_height']) & 
            (label <= self.height_filter['max_height'])
        )
        
        # åˆ›å»ºæœ€ç»ˆçš„æœ‰æ•ˆæ©ç 
        final_valid_mask = valid_mask & height_valid_mask
        
        # å½’ä¸€åŒ–
        normalized_label = np.full_like(label, -1.0)  # æ— æ•ˆå€¼æ ‡è®°ä¸º-1
        
        if np.any(final_valid_mask):
            valid_heights = label[final_valid_mask]
            normalized_heights = self.height_normalizer.normalize(valid_heights)
            normalized_label[final_valid_mask] = np.clip(normalized_heights, 0, 1)
        
        return normalized_label
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'min_height': self.global_min,
            'max_height': self.global_max,
            'mean_height': self.global_mean,
            'std_height': self.global_std,
            'height_filter': self.height_filter,
            'normalization_method': self.normalization_method,
            'total_samples': len(self.file_pairs)
        }
    
    def get_normalizer(self):
        """è·å–å½’ä¸€åŒ–å™¨"""
        return self.height_normalizer


# ç®€åŒ–çš„ä¾¿åˆ©å‡½æ•°
def create_gamus_dataloader(image_dir, label_dir, batch_size=8, shuffle=True, 
                           normalization_method='percentile', enable_augmentation=False,
                           stats_json_path=None, height_filter=None, 
                           force_recompute=False, num_workers=4):
    """
    åˆ›å»ºGAMUSæ•°æ®åŠ è½½å™¨
    
    å‚æ•°:
        image_dir: å½±åƒç›®å½•
        label_dir: æ ‡ç­¾ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        normalization_method: å½’ä¸€åŒ–æ–¹æ³•
        enable_augmentation: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
        stats_json_path: ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„
        height_filter: é«˜åº¦è¿‡æ»¤å™¨ï¼Œä¾‹å¦‚ {'min_height': -5.0, 'max_height': 100.0}
        force_recompute: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    """
    dataset = GAMUSDataset(
        image_dir=image_dir,
        label_dir=label_dir,
        normalization_method=normalization_method,
        enable_augmentation=enable_augmentation,
        stats_json_path=stats_json_path,
        height_filter=height_filter,
        force_recompute=force_recompute
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True if shuffle else False
    )
    dataloader.height_normalizer = dataset.height_normalizer
    return dataloader, dataset


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # è®¾ç½®é«˜åº¦è¿‡æ»¤å™¨ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    height_filter = {
        'min_height': -5.0,   # æœ€å°é«˜åº¦ï¼š-5ç±³
        'max_height': 100.0   # æœ€å¤§é«˜åº¦ï¼š100ç±³ï¼ˆè€Œä¸æ˜¯390ç±³ï¼‰
    }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader, dataset = create_gamus_dataloader(
        image_dir='/path/to/images',
        label_dir='/path/to/labels',
        batch_size=8,
        shuffle=True,
        normalization_method='percentile',
        enable_augmentation=True,
        stats_json_path='./gamus_stats.json',
        height_filter=height_filter,  # åº”ç”¨é«˜åº¦è¿‡æ»¤
        force_recompute=False
    )
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    stats = dataset.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    for batch_idx, (images, labels) in enumerate(dataloader):
        print(f"æ‰¹æ¬¡ {batch_idx}: å½±åƒ {images.shape}, æ ‡ç­¾ {labels.shape}")
        if batch_idx >= 2:  # åªæµ‹è¯•å‰å‡ ä¸ªæ‰¹æ¬¡
            break